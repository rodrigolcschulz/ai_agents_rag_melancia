{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Fine-Tuning QLoRA - Google Colab\n",
        "\n",
        "Este notebook implementa fine-tuning de LLMs usando QLoRA (Quantized LoRA).\n",
        "\n",
        "## üìã Pr√©-requisitos\n",
        "\n",
        "1. ‚úÖ Google Colab com **GPU T4** (gratuito)\n",
        "2. ‚úÖ Dataset preparado (use `prepare_finetuning_data.ipynb`)\n",
        "3. ‚úÖ Conta HuggingFace (para baixar modelos)\n",
        "\n",
        "## üéØ O que este notebook faz:\n",
        "\n",
        "1. Configura ambiente com GPU\n",
        "2. Instala depend√™ncias QLoRA\n",
        "3. Carrega dataset do Google Drive\n",
        "4. Carrega modelo base com quantiza√ß√£o 4-bit\n",
        "5. Aplica LoRA adapters\n",
        "6. Fine-tuna com seu dataset\n",
        "7. Salva modelo treinado\n",
        "8. Exporta para usar localmente\n",
        "\n",
        "## ‚è±Ô∏è Tempo estimado: 30 min - 2 horas\n",
        "\n",
        "Depende do tamanho do modelo e dataset.\n",
        "\n",
        "## üí° Dica\n",
        "\n",
        "Execute c√©lula por c√©lula e monitore uso de GPU/RAM.\n",
        "\n",
        "---\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANTE: Ative GPU no Colab**\n",
        "\n",
        "`Runtime > Change runtime type > Hardware accelerator > T4 GPU`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Verificar GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nüî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Instalar Depend√™ncias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q -U transformers accelerate peft bitsandbytes datasets trl\n",
        "!pip install -q -U wandb tensorboard  # Opcional: para tracking\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê Autentica√ß√£o HuggingFace\n",
        "\n",
        "Para baixar modelos privados (como Llama 2).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Tentar obter token do Colab Secrets\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"‚úÖ Autenticado no HuggingFace via Colab Secret!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel acessar HF_TOKEN dos secrets: {e}\")\n",
        "    print(\"‚ö†Ô∏è Apenas modelos p√∫blicos dispon√≠veis\")\n",
        "    print(\"\\nüí° Dica: Adicione HF_TOKEN nos Secrets do Colab:\")\n",
        "    print(\"   üîë √çcone de chave no menu lateral ‚Üí Add Secret ‚Üí Nome: HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Montar Google Drive\n",
        "\n",
        "Para acessar seu dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Google Drive montado!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configura√ß√£o do Treinamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CONFIGURA√á√ïES - AJUSTE AQUI\n",
        "# ========================================\n",
        "\n",
        "# Modelo base (escolha um)\n",
        "MODEL_NAME = \"microsoft/phi-2\"  # 2.7B - r√°pido, bom para testes\n",
        "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # 7B - melhor qualidade, mais lento\n",
        "# MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"  # 7B - excelente qualidade\n",
        "\n",
        "# Caminho do dataset no Google Drive\n",
        "DATASET_PATH = \"/content/drive/MyDrive/datascience/data/training_dataset\"  # Ajuste!\n",
        "\n",
        "# Nome do modelo final\n",
        "OUTPUT_MODEL_NAME = \"phi2-retail-media\"\n",
        "\n",
        "# Diret√≥rio de sa√≠da\n",
        "OUTPUT_DIR = \"./results\"\n",
        "\n",
        "# Par√¢metros de treinamento\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 1  # ‚ö° Otimizado para T4 (15GB)\n",
        "GRADIENT_ACCUMULATION = 16  # Mant√©m effective batch = 16\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_SEQ_LENGTH = 1024  # ‚ö° Reduzido para economizar mem√≥ria (antes: 2048)\n",
        "\n",
        "# LoRA config\n",
        "LORA_R = 8  # ‚ö° Reduzido para economizar mem√≥ria (antes: 16)\n",
        "LORA_ALPHA = 16  # Mant√©m propor√ß√£o 2:1 com R\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Logging\n",
        "USE_WANDB = False  # True para usar Weights & Biases\n",
        "WANDB_PROJECT = \"qlora-finetuning\"\n",
        "\n",
        "print(\"‚úÖ Configura√ß√£o definida!\")\n",
        "print(f\"üì¶ Modelo: {MODEL_NAME}\")\n",
        "print(f\"üìÅ Dataset: {DATASET_PATH}\")\n",
        "print(f\"üéØ Output: {OUTPUT_MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Carregar Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk, load_dataset\n",
        "\n",
        "# Carregar dataset do Google Drive\n",
        "try:\n",
        "    dataset = load_from_disk(DATASET_PATH)\n",
        "    print(f\"‚úÖ Dataset carregado do Google Drive!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è N√£o foi poss√≠vel carregar do Drive, usando dataset de exemplo...\")\n",
        "    # Criar dataset de exemplo para testes\n",
        "    from datasets import Dataset\n",
        "    \n",
        "    sample_data = [\n",
        "        {\"text\": \"### Instruction:\\nO que √© Retail Media?\\n\\n### Response:\\nRetail Media √© uma forma de publicidade digital onde varejistas monetizam seus sites...\"},\n",
        "        {\"text\": \"### Instruction:\\nExplique RTB\\n\\n### Response:\\nRTB (Real-Time Bidding) √© um processo de compra automatizada de an√∫ncios...\"},\n",
        "        # Adicione mais exemplos ou carregue de outro lugar\n",
        "    ]\n",
        "    \n",
        "    dataset_dict = {\"train\": Dataset.from_list(sample_data), \"test\": Dataset.from_list(sample_data[:1])}\n",
        "    from datasets import DatasetDict\n",
        "    dataset = DatasetDict(dataset_dict)\n",
        "\n",
        "print(f\"\\nüìä Dataset info:\")\n",
        "print(f\"   Train: {len(dataset['train'])} exemplos\")\n",
        "if 'test' in dataset:\n",
        "    print(f\"   Test: {len(dataset['test'])} exemplos\")\n",
        "\n",
        "print(f\"\\nüìù Exemplo:\")\n",
        "print(dataset['train'][0]['text'][:200] + \"...\")\n",
        "\n",
        "# ‚ö†Ô∏è AVISO DE DATASET PEQUENO\n",
        "train_size = len(dataset['train'])\n",
        "print(f\"\\n{'='*80}\")\n",
        "if train_size < 500:\n",
        "    print(f\"‚ö†Ô∏è  ATEN√á√ÉO: Dataset muito pequeno!\")\n",
        "    print(f\"   Voc√™ tem apenas {train_size} exemplos de treino\")\n",
        "    print(f\"   Para fine-tuning efetivo, recomenda-se:\")\n",
        "    print(f\"   ‚Ä¢ M√≠nimo: 500-1000 exemplos\")\n",
        "    print(f\"   ‚Ä¢ Ideal: 2000-5000 exemplos\")\n",
        "    print(f\"\\n   Com poucos exemplos, o modelo pode:\")\n",
        "    print(f\"   ‚ùå Overfitar completamente\")\n",
        "    print(f\"   ‚ùå Esquecer conhecimento geral (catastrophic forgetting)\")\n",
        "    print(f\"   ‚ùå Gerar respostas sem sentido\")\n",
        "    print(f\"\\n   üí° Alternativas:\")\n",
        "    print(f\"   1. Use apenas RAG (sem fine-tuning) - geralmente funciona melhor!\")\n",
        "    print(f\"   2. Aumente o dataset usando data augmentation\")\n",
        "    print(f\"   3. Use um modelo j√° especializado (ex: Llama-2-chat)\")\n",
        "else:\n",
        "    print(f\"‚úÖ Tamanho do dataset adequado: {train_size} exemplos\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Carregar Modelo com Quantiza√ß√£o 4-bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"üîÑ Carregando modelo e tokenizer...\")\n",
        "\n",
        "# Configurar quantiza√ß√£o 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Carregar modelo\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_cache=False,  # Necess√°rio para gradient checkpointing\n",
        ")\n",
        "\n",
        "# Preparar para treinamento k-bit\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Carregar tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Configurar padding\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "print(\"‚úÖ Modelo e tokenizer carregados!\")\n",
        "print(f\"üìä Modelo: {MODEL_NAME}\")\n",
        "print(f\"üíæ Tamanho: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Aplicar LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Ajuste para seu modelo\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Aplicar LoRA ao modelo\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Mostrar par√¢metros trein√°veis\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"\\n‚úÖ LoRA aplicado!\")\n",
        "print(f\"üéØ Rank: {LORA_R}\")\n",
        "print(f\"üìä Alpha: {LORA_ALPHA}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî§ Tokenizar Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    outputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
        "    return outputs\n",
        "\n",
        "print(\"üîÑ Tokenizando dataset...\")\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    desc=\"Tokenizing\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dataset tokenizado!\")\n",
        "print(f\"   Train: {len(tokenized_dataset['train'])} exemplos\")\n",
        "if 'test' in tokenized_dataset:\n",
        "    print(f\"   Test: {len(tokenized_dataset['test'])} exemplos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Configurar e Iniciar Treinamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\" if 'test' in tokenized_dataset else \"no\",  # Mudou de evaluation_strategy para eval_strategy\n",
        "    eval_steps=100 if 'test' in tokenized_dataset else None,\n",
        "    save_total_limit=3,\n",
        "    seed=42,\n",
        "    report_to=\"wandb\" if USE_WANDB else \"none\",\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Criar trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset.get(\"test\"),\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configurado!\")\n",
        "print(f\"\\nüìä Par√¢metros de treinamento:\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
        "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "\n",
        "print(\"\\nüöÄ Iniciando treinamento...\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Otimizar Mem√≥ria GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üßπ LIMPAR MEM√ìRIA GPU ANTES DE TREINAR\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Configurar vari√°vel de ambiente para fragmenta√ß√£o de mem√≥ria\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Limpar cache da GPU\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Mostrar mem√≥ria dispon√≠vel\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üíæ GPU Memory antes do treino:\")\n",
        "    print(f\"   Alocada: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"   Reservada: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"   Livre: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"‚úÖ Mem√≥ria otimizada! Pronto para treinar.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TREINAR!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Salvar Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Criar diret√≥rio de sa√≠da\n",
        "final_output_dir = f\"./models/{OUTPUT_MODEL_NAME}\"\n",
        "os.makedirs(final_output_dir, exist_ok=True)\n",
        "\n",
        "# Salvar apenas adaptadores LoRA (muito menor!)\n",
        "model.save_pretrained(final_output_dir)\n",
        "tokenizer.save_pretrained(final_output_dir)\n",
        "\n",
        "print(f\"‚úÖ Modelo salvo em: {final_output_dir}\")\n",
        "print(f\"üì¶ Tamanho dos adaptadores: ~{sum(os.path.getsize(os.path.join(dirpath,filename)) for dirpath, dirnames, filenames in os.walk(final_output_dir) for filename in filenames) / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "# Copiar para Google Drive (backup)\n",
        "drive_output = f\"/content/drive/MyDrive/datascience/finetuned_models/{OUTPUT_MODEL_NAME}\"\n",
        "!mkdir -p \"{drive_output}\"\n",
        "!cp -r \"{final_output_dir}\"/* \"{drive_output}/\"\n",
        "\n",
        "print(f\"\\n‚úÖ Backup salvo no Google Drive: {drive_output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è IMPORTANTE: Antes de testar\n",
        "\n",
        "Se seu dataset tem **menos de 500 exemplos**, o modelo provavelmente vai gerar respostas sem sentido!\n",
        "\n",
        "### üî¥ Sintomas de dataset pequeno:\n",
        "- Respostas sobre t√≥picos completamente diferentes (m√∫sica, matem√°tica, etc)\n",
        "- Texto repetitivo ou sem coer√™ncia\n",
        "- Mistura de l√≠nguas ou conceitos aleat√≥rios\n",
        "- C√≥digo Python aparecendo nas respostas\n",
        "\n",
        "### ‚úÖ Solu√ß√£o:\n",
        "\n",
        "**Op√ß√£o 1 (RECOMENDADO):** Use RAG sem fine-tuning\n",
        "- Seu RAG j√° funciona bem com os 107 markdowns\n",
        "- Sem risco de catastrophic forgetting\n",
        "- Mais f√°cil de manter\n",
        "\n",
        "**Op√ß√£o 2:** Aumente o dataset para 2000+ exemplos\n",
        "- Veja: `docs/FINE_TUNING_VS_RAG.md`\n",
        "- Use data augmentation com GPT-4\n",
        "- Colete mais dados reais\n",
        "\n",
        "**Op√ß√£o 3:** N√£o teste agora, apenas salve o modelo\n",
        "- S√≥ teste depois de aumentar o dataset\n",
        "- Ou use este modelo como experimento/aprendizado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Testar Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(prompt, max_length=512):\n",
        "    \"\"\"Gera resposta usando o modelo fine-tunado.\"\"\"\n",
        "    \n",
        "    # Formatar prompt (Alpaca format)\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "    \n",
        "    # Tokenizar\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Gerar\n",
        "    with torch.no_grad():  # Desabilitar gradientes durante infer√™ncia\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1,  # Evita repeti√ß√£o\n",
        "        )\n",
        "    \n",
        "    # Decodificar\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extrair apenas a resposta\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[1].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "# üîß Preparar modelo para infer√™ncia\n",
        "print(\"üîß Preparando modelo para infer√™ncia...\")\n",
        "model.eval()  # Modo de avalia√ß√£o\n",
        "model.config.use_cache = True  # Habilitar cache para infer√™ncia\n",
        "\n",
        "# Testar\n",
        "test_prompts = [\n",
        "    \"Como funciona o cat√°logo no Mercado Livre?\",\n",
        "    \"Explique o conceito de ACOS\",\n",
        "    \"Como dominar as vendas nos marketplaces?\",\n",
        "]\n",
        "\n",
        "print(\"üß™ Testando modelo fine-tunado:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nüìù Pergunta: {prompt}\")\n",
        "    print(f\"üí¨ Resposta: {generate_response(prompt)}\")\n",
        "    print(\"-\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Comprimir para Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprimir modelo para facilitar download\n",
        "!cd models && zip -r {OUTPUT_MODEL_NAME}.zip {OUTPUT_MODEL_NAME}\n",
        "\n",
        "print(f\"‚úÖ Modelo comprimido: models/{OUTPUT_MODEL_NAME}.zip\")\n",
        "print(f\"\\nüì• Para usar localmente:\")\n",
        "print(f\"   1. Baixe o arquivo ZIP\")\n",
        "print(f\"   2. Descompacte na sua m√°quina\")\n",
        "print(f\"   3. Use o c√≥digo em export_to_ollama.py para converter para Ollama\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Resumo e Pr√≥ximos Passos\n",
        "\n",
        "### ‚úÖ O que fizemos:\n",
        "\n",
        "1. ‚úÖ Configuramos ambiente com GPU\n",
        "2. ‚úÖ Instalamos depend√™ncias QLoRA\n",
        "3. ‚úÖ Carregamos modelo base com quantiza√ß√£o 4-bit\n",
        "4. ‚úÖ Aplicamos LoRA adapters\n",
        "5. ‚úÖ Fine-tunamos com seu dataset\n",
        "6. ‚úÖ Salvamos modelo treinado\n",
        "7. ‚úÖ Testamos o modelo\n",
        "\n",
        "### üì• Arquivos gerados:\n",
        "\n",
        "- **Adaptadores LoRA**: `models/{OUTPUT_MODEL_NAME}/` (~50-200 MB)\n",
        "- **Backup no Drive**: Salvo automaticamente\n",
        "- **ZIP para download**: `models/{OUTPUT_MODEL_NAME}.zip`\n",
        "\n",
        "### üè† Pr√≥ximos passos (localmente):\n",
        "\n",
        "1. **Download**: Baixe o modelo treinado do Colab/Drive\n",
        "2. **Merge**: Use `export_to_ollama.py` para fazer merge com modelo base\n",
        "3. **Converter**: Converta para GGUF usando llama.cpp (opcional)\n",
        "4. **Ollama**: Crie Modelfile e use com `ollama run`\n",
        "\n",
        "### üí° Dicas:\n",
        "\n",
        "- **Qualidade**: Se respostas n√£o est√£o boas, treine mais epochs ou aumente dataset\n",
        "- **Overfitting**: Se modelo decorou respostas, reduza epochs ou aumente dataset\n",
        "- **Mem√≥ria**: Se ficou sem GPU memory, reduza `BATCH_SIZE` ou `MAX_SEQ_LENGTH`\n",
        "- **Velocidade**: Modelos menores (Phi-2) s√£o mais r√°pidos que 7B\n",
        "\n",
        "### üìö Recursos:\n",
        "\n",
        "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
        "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
        "- [Transformers Documentation](https://huggingface.co/docs/transformers)\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
