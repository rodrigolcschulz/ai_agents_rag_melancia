{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Evaluation Loops - Avalia√ß√£o de Modelos Fine-Tunados\n",
    "\n",
    "Este notebook implementa **evaluation loops profissionais** para avaliar modelos fine-tunados.\n",
    "\n",
    "## üéØ O que este notebook faz:\n",
    "\n",
    "1. ‚úÖ **Carrega modelos** (base vs fine-tuned)\n",
    "2. ‚úÖ **M√©tricas autom√°ticas** (ROUGE, BLEU, BERTScore, Perplexity)\n",
    "3. ‚úÖ **Compara√ß√£o lado a lado** (qual modelo responde melhor?)\n",
    "4. ‚úÖ **Benchmark de dom√≠nio** (perguntas de Retail Media)\n",
    "5. ‚úÖ **Relat√≥rios e visualiza√ß√µes**\n",
    "6. ‚úÖ **Integra√ß√£o com MLflow** (tracking de experimentos)\n",
    "\n",
    "## üìã Pr√©-requisitos:\n",
    "\n",
    "- ‚úÖ Modelo fine-tunado salvo (do notebook `fine_tuning_qlora_colab.ipynb`)\n",
    "- ‚úÖ Dataset de teste com respostas de refer√™ncia\n",
    "- ‚úÖ GPU recomendada (pode rodar em CPU mas √© lento)\n",
    "\n",
    "## üè¢ Boas Pr√°ticas:\n",
    "\n",
    "Este notebook segue padr√µes de empresas como OpenAI, Google, Anthropic:\n",
    "- ‚úÖ Separa√ß√£o treino/avalia√ß√£o\n",
    "- ‚úÖ M√©tricas objetivas e reproduz√≠veis\n",
    "- ‚úÖ Tracking de experimentos\n",
    "- ‚úÖ Compara√ß√£o sistem√°tica\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Instalar Depend√™ncias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias (se necess√°rio)\n",
    "%pip install -q transformers accelerate peft bitsandbytes\n",
    "%pip install -q rouge-score bert-score nltk sacrebleu\n",
    "%pip install -q pandas matplotlib seaborn\n",
    "%pip install -q mlflow\n",
    "\n",
    "print(\"‚úÖ Depend√™ncias instaladas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Imports e Configura√ß√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "from sacrebleu.metrics import BLEU\n",
    "import mlflow\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas!\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers: {__import__('transformers').__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configura√ß√µes - **AJUSTE AQUI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURA√á√ïES - AJUSTE AQUI\n",
    "# ========================================\n",
    "\n",
    "# Modelo base (mesmo usado no fine-tuning)\n",
    "BASE_MODEL_NAME = \"microsoft/phi-2\"\n",
    "\n",
    "# Caminho do modelo fine-tunado (adaptadores LoRA)\n",
    "FINETUNED_MODEL_PATH = \"./models/phi2-retail-media\"\n",
    "# No Colab: \"/content/drive/MyDrive/finetuned_models/phi2-retail-media\"\n",
    "\n",
    "# Dataset de teste com respostas de refer√™ncia\n",
    "TEST_DATASET_PATH = \"./data/evaluation/test_questions.json\"\n",
    "\n",
    "# Diret√≥rio de sa√≠da\n",
    "OUTPUT_DIR = \"./evaluation_results\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MLflow\n",
    "EXPERIMENT_NAME = \"model-evaluation\"\n",
    "\n",
    "# Par√¢metros de gera√ß√£o\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "print(\"‚úÖ Configura√ß√£o definida!\")\n",
    "print(f\"üì¶ Modelo base: {BASE_MODEL_NAME}\")\n",
    "print(f\"üéØ Modelo fine-tuned: {FINETUNED_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Carregar Classe ModelEvaluator\n",
    "\n",
    "Usaremos a classe reutiliz√°vel do m√≥dulo `src/evaluation/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar src ao path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.evaluation.evaluator import ModelEvaluator, compare_models\n",
    "\n",
    "print(\"‚úÖ ModelEvaluator carregado!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Criar Dataset de Teste\n",
    "\n",
    "Formato esperado:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"question\": \"O que √© ACOS?\",\n",
    "    \"reference_answer\": \"ACOS (Advertising Cost of Sale) √©...\",\n",
    "    \"category\": \"metricas\"  // opcional\n",
    "  }\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset de exemplo se n√£o existir\n",
    "if not Path(TEST_DATASET_PATH).exists():\n",
    "    print(\"‚ö†Ô∏è Dataset n√£o encontrado. Criando exemplo...\")\n",
    "    \n",
    "    example_data = [\n",
    "        {\n",
    "            \"question\": \"O que √© Retail Media?\",\n",
    "            \"reference_answer\": \"Retail Media √© uma forma de publicidade digital onde varejistas monetizam seus sites e aplicativos vendendo espa√ßos publicit√°rios para marcas e fabricantes.\",\n",
    "            \"category\": \"conceitos\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Explique o que √© ACOS\",\n",
    "            \"reference_answer\": \"ACOS (Advertising Cost of Sale) √© uma m√©trica que indica quanto voc√™ gasta em publicidade para cada real de venda gerada. Calcula-se dividindo o custo da publicidade pela receita gerada.\",\n",
    "            \"category\": \"metricas\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"O que √© RTB?\",\n",
    "            \"reference_answer\": \"RTB (Real-Time Bidding) √© um processo de compra automatizada de espa√ßos publicit√°rios em tempo real atrav√©s de leil√µes.\",\n",
    "            \"category\": \"conceitos\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    Path(TEST_DATASET_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(TEST_DATASET_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(example_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset exemplo criado: {TEST_DATASET_PATH}\")\n",
    "    print(\"üí° Substitua por seu dataset real!\")\n",
    "\n",
    "# Carregar dataset\n",
    "with open(TEST_DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"\\nüìä Dataset carregado: {len(test_data)} exemplos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Carregar Modelos (Base + Fine-Tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Carregando modelos...\\\\n\")\n",
    "\n",
    "# Configurar quantiza√ß√£o 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 1. Carregar modelo BASE\n",
    "print(\"üîµ Carregando modelo BASE...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Carregar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Modelo base carregado: {BASE_MODEL_NAME}\")\n",
    "\n",
    "# 2. Carregar modelo FINE-TUNED\n",
    "print(\"\\\\nüü¢ Carregando modelo FINE-TUNED...\")\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINETUNED_MODEL_PATH\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo fine-tuned carregado: {FINETUNED_MODEL_PATH}\")\n",
    "print(\"\\\\n‚úÖ Ambos os modelos prontos para avalia√ß√£o!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Criar Evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Criando evaluators...\\\\n\")\n",
    "\n",
    "# Evaluator para modelo base\n",
    "base_evaluator = ModelEvaluator(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=\"base\",\n",
    "    prompt_format=\"alpaca\"\n",
    ")\n",
    "\n",
    "# Evaluator para modelo fine-tuned  \n",
    "finetuned_evaluator = ModelEvaluator(\n",
    "    model=finetuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=\"finetuned\",\n",
    "    prompt_format=\"alpaca\"\n",
    ")\n",
    "\n",
    "print(\"\\\\n‚úÖ Evaluators prontos para uso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ EVALUATION LOOP - Avaliar Dataset Completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üöÄ INICIANDO EVALUATION LOOP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Avaliar modelo base\n",
    "print(\"\\\\nüîµ Avaliando modelo BASE...\")\n",
    "base_results = base_evaluator.evaluate_dataset(\n",
    "    test_data,\n",
    "    verbose=True,\n",
    "    save_path=f\"{OUTPUT_DIR}/base_results.csv\"\n",
    ")\n",
    "\n",
    "# Avaliar modelo fine-tuned\n",
    "print(\"\\\\nüü¢ Avaliando modelo FINE-TUNED...\")\n",
    "finetuned_results = finetuned_evaluator.evaluate_dataset(\n",
    "    test_data,\n",
    "    verbose=True,\n",
    "    save_path=f\"{OUTPUT_DIR}/finetuned_results.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EVALUATION LOOP CONCLU√çDO!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Compara√ß√£o de Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar resultados\n",
    "all_results = pd.concat([base_results, finetuned_results], ignore_index=True)\n",
    "\n",
    "# M√©tricas principais\n",
    "metric_columns = ['rouge1_f', 'rouge2_f', 'rougeL_f', 'bleu', 'bertscore_f1']\n",
    "\n",
    "# Calcular m√©dias por modelo\n",
    "comparison = all_results.groupby('model')[metric_columns].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\\\nüìä COMPARA√á√ÉO DE M√âTRICAS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.round(4))\n",
    "\n",
    "# Calcular melhorias\n",
    "print(\"\\\\nüìà MELHORIA DO FINE-TUNING:\")\n",
    "print(\"=\"*80)\n",
    "for metric in metric_columns:\n",
    "    base_score = base_results[metric].mean()\n",
    "    finetuned_score = finetuned_results[metric].mean()\n",
    "    improvement = ((finetuned_score - base_score) / base_score) * 100\n",
    "    symbol = \"‚úÖ\" if improvement > 0 else \"‚ö†Ô∏è\"\n",
    "    print(f\"{symbol} {metric:20s}: {improvement:+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualiza√ß√µes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de compara√ß√£o\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "means = all_results.groupby('model')[metric_columns].mean()\n",
    "means.T.plot(kind='bar', figsize=(12, 6), rot=45)\n",
    "\n",
    "plt.title('Compara√ß√£o de M√©tricas: Base vs Fine-Tuned', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('M√©trica', fontsize=11)\n",
    "plt.ylabel('Score', fontsize=11)\n",
    "plt.legend(['Modelo Base', 'Modelo Fine-Tuned'])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/metrics_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Gr√°fico salvo: {OUTPUT_DIR}/metrics_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Integra√ß√£o com MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar MLflow\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Log resultados\n",
    "with mlflow.start_run(run_name=f\"eval_{timestamp}\"):\n",
    "    # Par√¢metros\n",
    "    mlflow.log_param(\"base_model\", BASE_MODEL_NAME)\n",
    "    mlflow.log_param(\"finetuned_model\", FINETUNED_MODEL_PATH)\n",
    "    mlflow.log_param(\"test_samples\", len(test_data))\n",
    "    \n",
    "    # M√©tricas do modelo base\n",
    "    for metric in metric_columns:\n",
    "        mlflow.log_metric(f\"base_{metric}\", base_results[metric].mean())\n",
    "    \n",
    "    # M√©tricas do modelo fine-tuned\n",
    "    for metric in metric_columns:\n",
    "        mlflow.log_metric(f\"finetuned_{metric}\", finetuned_results[metric].mean())\n",
    "    \n",
    "    # Melhorias\n",
    "    for metric in metric_columns:\n",
    "        improvement = ((finetuned_results[metric].mean() - base_results[metric].mean()) / base_results[metric].mean()) * 100\n",
    "        mlflow.log_metric(f\"improvement_{metric}\", improvement)\n",
    "    \n",
    "    # Artifacts\n",
    "    mlflow.log_artifact(f\"{OUTPUT_DIR}/base_results.csv\")\n",
    "    mlflow.log_artifact(f\"{OUTPUT_DIR}/finetuned_results.csv\")\n",
    "    mlflow.log_artifact(f\"{OUTPUT_DIR}/metrics_comparison.png\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Resultados logados no MLflow!\")\n",
    "print(\"üî¨ Para visualizar: mlflow ui --port 5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Resumo Final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"üìã RESUMO DA AVALIA√á√ÉO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_improvement = sum([\n",
    "    ((finetuned_results[m].mean() - base_results[m].mean()) / base_results[m].mean()) * 100\n",
    "    for m in metric_columns\n",
    "]) / len(metric_columns)\n",
    "\n",
    "print(f\"\\\\nüéØ Melhoria M√©dia Geral: {avg_improvement:+.2f}%\")\n",
    "\n",
    "if avg_improvement > 5:\n",
    "    print(\"\\\\n‚úÖ CONCLUS√ÉO: Fine-tuning foi EFETIVO! üéâ\")\n",
    "    print(\"   O modelo fine-tuned tem performance significativamente melhor.\")\n",
    "elif avg_improvement > 0:\n",
    "    print(\"\\\\n‚ö†Ô∏è CONCLUS√ÉO: Fine-tuning teve melhoria LEVE.\")\n",
    "    print(\"   Considere: mais dados de treino, mais epochs, ou ajustar hyperpar√¢metros.\")\n",
    "else:\n",
    "    print(\"\\\\n‚ùå CONCLUS√ÉO: Fine-tuning n√£o melhorou.\")\n",
    "    print(\"   Revisar: qualidade do dataset, formato dos dados, hyperpar√¢metros.\")\n",
    "\n",
    "print(f\"\\\\nüìÅ Arquivos gerados em: {OUTPUT_DIR}/\")\n",
    "print(f\"   - base_results.csv\")\n",
    "print(f\"   - finetuned_results.csv\")\n",
    "print(f\"   - metrics_comparison.png\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AVALIA√á√ÉO COMPLETA CONCLU√çDA!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
