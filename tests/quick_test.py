#!/usr/bin/env python3
"""
Script de teste r√°pido dos LLMs open source
Testa conectividade e performance b√°sica
"""
import sys
import time
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.experiments.multi_llm import MultiLLMManager

# Cores para terminal
GREEN = '\033[0;32m'
YELLOW = '\033[1;33m'
RED = '\033[0;31m'
BLUE = '\033[0;34m'
NC = '\033[0m'

def print_header(text):
    """Imprime cabe√ßalho formatado"""
    print(f"\n{'='*60}")
    print(f"{BLUE}{text}{NC}")
    print(f"{'='*60}\n")

def test_model(provider, model_name, question):
    """
    Testa um modelo espec√≠fico
    
    Args:
        provider: Nome do provedor (openai, ollama, huggingface)
        model_name: Nome do modelo
        question: Pergunta de teste
        
    Returns:
        Dict com resultados ou None se falhar
    """
    print(f"{YELLOW}ü§ñ Testando: {provider.upper()} - {model_name}{NC}")
    
    try:
        # Criar LLM
        print(f"   ‚öôÔ∏è  Inicializando...")
        llm = MultiLLMManager.create_llm(
            provider=provider,
            model_name=model_name,
            temperature=0.5,
            max_tokens=500
        )
        
        # Fazer pergunta
        print(f"   üí¨ Gerando resposta...")
        start_time = time.time()
        response = llm.invoke(question)
        latency = time.time() - start_time
        
        # Extrair texto da resposta
        if hasattr(response, 'content'):
            answer = response.content
        else:
            answer = str(response)
        
        # Estat√≠sticas
        words = len(answer.split())
        chars = len(answer)
        
        print(f"{GREEN}   ‚úì Sucesso!{NC}")
        print(f"   ‚è±Ô∏è  Lat√™ncia: {latency:.2f}s")
        print(f"   üìù Tamanho: {words} palavras, {chars} caracteres")
        print(f"\n{BLUE}   Resposta:{NC}")
        
        # Mostrar primeiras linhas
        lines = answer.split('\n')[:5]
        for line in lines:
            if line.strip():
                print(f"   {line[:100]}")
        
        if len(answer) > 500:
            print(f"   ... (resposta truncada)")
        
        print()
        
        return {
            'provider': provider,
            'model': model_name,
            'success': True,
            'latency': latency,
            'words': words,
            'answer': answer
        }
        
    except Exception as e:
        print(f"{RED}   ‚úó Erro: {e}{NC}")
        print()
        return {
            'provider': provider,
            'model': model_name,
            'success': False,
            'error': str(e)
        }

def main():
    """Fun√ß√£o principal"""
    print_header("üçâ Mel√¢ncIA - Teste R√°pido de LLMs")
    
    # Pergunta de teste (contexto Retail Media)
    question = "O que √© Retail Media e quais s√£o suas principais m√©tricas?"
    
    print(f"{BLUE}Pergunta de teste:{NC} {question}\n")
    
    # Modelos para testar (ordem de prioridade)
    models_to_test = [
        # Ollama (local - prioridade)
        ("ollama", "phi3:mini"),
        ("ollama", "llama3.2:3b"),
        ("ollama", "gemma2:2b"),
        
        # OpenAI (se configurado)
        ("openai", "gpt-4o-mini"),
    ]
    
    results = []
    
    for provider, model_name in models_to_test:
        result = test_model(provider, model_name, question)
        if result:
            results.append(result)
    
    # Resumo
    print_header("üìä Resumo dos Testes")
    
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    if successful:
        print(f"{GREEN}‚úÖ Modelos funcionando: {len(successful)}{NC}\n")
        
        # Ordenar por lat√™ncia
        successful.sort(key=lambda x: x['latency'])
        
        print(f"{'Modelo':<25} {'Lat√™ncia':<12} {'Palavras':<10} {'Status'}")
        print("-" * 60)
        
        for r in successful:
            model_full = f"{r['provider']}::{r['model']}"
            print(f"{model_full:<25} {r['latency']:.2f}s{' ':<7} {r['words']:<10} ‚úì")
        
        print()
        
        # Melhor modelo
        best = successful[0]
        print(f"{YELLOW}üèÜ Melhor performance:{NC} {best['provider']}::{best['model']} ({best['latency']:.2f}s)")
    
    if failed:
        print(f"\n{RED}‚ùå Modelos com erro: {len(failed)}{NC}\n")
        for r in failed:
            print(f"  ‚Ä¢ {r['provider']}::{r['model']}: {r.get('error', 'Unknown error')}")
    
    # Recomenda√ß√µes
    print(f"\n{YELLOW}üí° Pr√≥ximos passos:{NC}")
    
    if not successful:
        print(f"  {RED}1. Instale o Ollama:{NC}")
        print(f"     ./scripts/setup_ollama.sh")
        print(f"\n  {RED}2. Configure OpenAI API key (opcional):{NC}")
        print(f"     echo 'OPENAI_API_KEY=sk-...' >> .env")
    else:
        print(f"  {GREEN}1. Executar benchmark completo:{NC}")
        print(f"     python src/experiments/run_experiments.py --mode quick")
        print(f"\n  {GREEN}2. Usar no c√≥digo:{NC}")
        print(f"     from src.experiments.multi_llm import MultiLLMManager")
        print(f"     llm = MultiLLMManager.create_llm('ollama', 'phi3:mini')")
        print(f"\n  {GREEN}3. Experimentar no Jupyter:{NC}")
        print(f"     jupyter lab")
        print(f"     # Abrir: notebooks/experimentacao_llms.ipynb")
    
    print()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n\n{YELLOW}‚ö†Ô∏è  Interrompido pelo usu√°rio{NC}")
        sys.exit(0)
    except Exception as e:
        print(f"\n{RED}‚ùå Erro fatal: {e}{NC}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

