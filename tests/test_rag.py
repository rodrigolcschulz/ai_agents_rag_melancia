#!/usr/bin/env python3
"""
Teste r√°pido do sistema RAG com Phi-3 Mini
Mostra a diferen√ßa entre usar o modelo puro vs com contexto dos documentos
"""
import sys
import time
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.experiments.multi_llm import MultiLLMManager
from src.agent.retriever import carregar_markdowns, get_retriever, indexar_novos_markdowns
from src.agent import config

# Cores
GREEN = '\033[0;32m'
YELLOW = '\033[1;33m'
BLUE = '\033[0;34m'
RED = '\033[0;31m'
NC = '\033[0m'

def print_header(text):
    print(f"\n{'='*70}")
    print(f"{BLUE}{text}{NC}")
    print(f"{'='*70}\n")

def test_without_rag(llm, question):
    """Testa modelo SEM contexto RAG"""
    print(f"{YELLOW}üìù Teste 1: SEM RAG (modelo puro){NC}\n")
    
    start = time.time()
    response = llm.invoke(question)
    latency = time.time() - start
    
    answer = response.content if hasattr(response, 'content') else str(response)
    
    print(f"{BLUE}Resposta:{NC}")
    print(f"{answer}\n")
    print(f"‚è±Ô∏è  Lat√™ncia: {latency:.2f}s")
    print(f"üìä Tamanho: {len(answer.split())} palavras")
    
    return answer, latency

def test_with_rag(llm, retriever, question):
    """Testa modelo COM contexto RAG"""
    print(f"\n{YELLOW}üìö Teste 2: COM RAG (modelo + documentos){NC}\n")
    
    # Buscar contexto relevante
    print(f"üîç Buscando contexto nos documentos...")
    docs = retriever.get_relevant_documents(question)
    
    print(f"{GREEN}‚úì Encontrados {len(docs)} documentos relevantes{NC}\n")
    
    # Mostrar fontes
    print(f"{BLUE}üìÑ Fontes encontradas:{NC}")
    for i, doc in enumerate(docs[:3], 1):
        source = doc.metadata.get('source', 'Desconhecido')
        filename = Path(source).name if source != 'Desconhecido' else source
        preview = doc.page_content[:100].replace('\n', ' ')
        print(f"  {i}. {filename}")
        print(f"     Preview: {preview}...")
    
    # Criar prompt com contexto
    context = "\n\n---\n\n".join([doc.page_content for doc in docs])
    
    prompt_with_context = f"""Com base nos seguintes documentos sobre Retail Media, responda a pergunta:

CONTEXTO:
{context}

PERGUNTA: {question}

RESPOSTA:"""
    
    print(f"\nüí¨ Gerando resposta com contexto...")
    start = time.time()
    response = llm.invoke(prompt_with_context)
    latency = time.time() - start
    
    answer = response.content if hasattr(response, 'content') else str(response)
    
    print(f"\n{BLUE}Resposta:{NC}")
    print(f"{answer}\n")
    print(f"‚è±Ô∏è  Lat√™ncia: {latency:.2f}s")
    print(f"üìä Tamanho: {len(answer.split())} palavras")
    
    return answer, latency

def main():
    print_header("üçâ Mel√¢ncIA - Teste RAG com Phi-3 Mini")
    
    # Pergunta de teste
    question = "O que √© Retail Media?"
    print(f"{BLUE}‚ùì Pergunta:{NC} {question}\n")
    
    # Inicializar LLM
    print(f"{YELLOW}ü§ñ Inicializando Phi-3 Mini...{NC}")
    try:
        llm = MultiLLMManager.create_llm(
            provider="ollama",
            model_name="phi3:mini",
            temperature=0.5,
            max_tokens=500
        )
        print(f"{GREEN}‚úì Phi-3 Mini carregado!{NC}\n")
    except Exception as e:
        print(f"{RED}‚ùå Erro ao carregar modelo: {e}{NC}")
        print(f"\n{YELLOW}üí° Certifique-se de que:{NC}")
        print(f"  1. Ollama est√° instalado")
        print(f"  2. Execute: ollama pull phi3:mini")
        return
    
    # ==========================================
    # TESTE 1: SEM RAG
    # ==========================================
    try:
        answer_no_rag, latency_no_rag = test_without_rag(llm, question)
    except Exception as e:
        print(f"{RED}‚ùå Erro no teste sem RAG: {e}{NC}")
        return
    
    # ==========================================
    # TESTE 2: COM RAG
    # ==========================================
    try:
        print(f"\n{YELLOW}üìö Preparando sistema RAG...{NC}")
        
        # Carregar e indexar documentos
        print(f"üìñ Carregando documentos markdown...")
        docs = carregar_markdowns(config.INPUT_MARKDOWN)
        print(f"{GREEN}‚úì {len(docs)} documentos carregados{NC}")
        
        print(f"üîç Indexando no banco vetorial...")
        indexar_novos_markdowns(docs, str(config.VECTOR_DB_DIR), config.EMBEDDING_MODEL)
        print(f"{GREEN}‚úì Documentos indexados{NC}")
        
        # Criar retriever
        print(f"‚öôÔ∏è  Criando retriever...")
        retriever = get_retriever(str(config.VECTOR_DB_DIR), config.EMBEDDING_MODEL)
        print(f"{GREEN}‚úì RAG pronto!{NC}")
        
        # Testar com RAG
        answer_with_rag, latency_with_rag = test_with_rag(llm, retriever, question)
        
    except Exception as e:
        print(f"{RED}‚ùå Erro no teste com RAG: {e}{NC}")
        import traceback
        traceback.print_exc()
        return
    
    # ==========================================
    # COMPARA√á√ÉO
    # ==========================================
    print_header("üìä Compara√ß√£o: SEM RAG vs COM RAG")
    
    print(f"{YELLOW}Lat√™ncia:{NC}")
    print(f"  ‚Ä¢ Sem RAG: {latency_no_rag:.2f}s")
    print(f"  ‚Ä¢ Com RAG: {latency_with_rag:.2f}s")
    
    print(f"\n{YELLOW}Tamanho da resposta:{NC}")
    print(f"  ‚Ä¢ Sem RAG: {len(answer_no_rag.split())} palavras")
    print(f"  ‚Ä¢ Com RAG: {len(answer_with_rag.split())} palavras")
    
    print(f"\n{GREEN}‚úÖ Conclus√£o:{NC}")
    print(f"  Com RAG, o modelo responde baseado nos seus {len(docs)} documentos!")
    print(f"  As respostas s√£o mais precisas e contextualizadas.")
    
    print(f"\n{YELLOW}üí° Pr√≥ximos passos:{NC}")
    print(f"  1. Adicione mais documentos markdown em: data/input/")
    print(f"  2. Execute o agente completo: python -m src.agent.main")
    print(f"  3. Use a interface web: python -m src.agent.web_interface")
    print()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n\n{YELLOW}‚ö†Ô∏è  Interrompido pelo usu√°rio{NC}")
        sys.exit(0)
    except Exception as e:
        print(f"\n{RED}‚ùå Erro fatal: {e}{NC}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

