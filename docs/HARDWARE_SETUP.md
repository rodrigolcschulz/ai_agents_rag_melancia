# ğŸ–¥ï¸ Guia de Setup - Hardware EspecÃ­fico

## ğŸ“Š EspecificaÃ§Ãµes do Sistema

- **CPU**: AMD Ryzen 5 3600X (6 cores, 12 threads, 2.2-3.8 GHz)
  - âœ… Processador razoÃ¡vel para LLMs pequenos/mÃ©dios
  
- **RAM**: 15 GB total (~10 GB disponÃ­vel)
  - âš ï¸ Limitado para LLMs grandes (>8B)
  - âœ… Suficiente para modelos 2-7B quantizados
  
- **GPU**: AMD Radeon RX 570/580 (Ellesmere)
  - âŒ Sem suporte CUDA (NVIDIA)
  - âš ï¸ ROCm disponÃ­vel mas complexo de configurar
  - ğŸ’¡ RecomendaÃ§Ã£o: Usar CPU para inferÃªncia
  
- **Armazenamento**: 439 GB total
  - Usado: 71 GB (17%)
  - DisponÃ­vel: 346 GB (79%)
  - âœ… EspaÃ§o suficiente para:
    - MÃºltiplos modelos LLM (2-5 GB cada)
    - Datasets de treinamento (1-10 GB)
    - Vector databases (ChromaDB: 100-500 MB)
    - Logs e experimentos MLflow (1-5 GB)
    - Modelos fine-tunados (2-10 GB cada)

### ğŸ’¾ Estimativa de Uso de Disco

| Item | Tamanho Estimado | Prioridade |
|------|------------------|------------|
| Ollama + 3 modelos (phi3, llama3.2, gemma2) | ~6 GB | Alta |
| ChromaDB vector database | 200-500 MB | Alta |
| MLflow experiments (6 meses) | 2-3 GB | MÃ©dia |
| Datasets RAG (markdown) | 50-200 MB | Alta |
| Fine-tuning datasets | 500 MB - 2 GB | Baixa |
| Modelos fine-tunados | 2-5 GB cada | Baixa |
| **Total estimado (uso completo)** | **15-25 GB** | - |

**âœ… ConclusÃ£o**: VocÃª tem **346 GB disponÃ­veis**, mais que suficiente para todas as fases do projeto, incluindo mÃºltiplos modelos e experimentos.

### ğŸ“Š Monitorar Uso de Disco

```bash
# Ver uso geral
df -h /

# Ver uso do projeto
du -sh /home/coneta/ai_agents_rag_melancia

# Ver tamanho dos modelos Ollama
du -sh ~/.ollama/models

# Ver tamanho do ChromaDB
du -sh /home/coneta/ai_agents_rag_melancia/data/vector_db

# Ver tamanho dos experimentos MLflow
du -sh /home/coneta/ai_agents_rag_melancia/mlruns
```

**Limpeza periÃ³dica:**
```bash
# Remover modelos nÃ£o utilizados do Ollama
ollama list
ollama rm <modelo_nÃ£o_usado>

# Limpar cache do Python
find . -type d -name __pycache__ -exec rm -rf {} +

# Limpar experimentos antigos do MLflow (opcional)
# Manter apenas Ãºltimos 3 meses
```

## âœ… O Que Ã‰ ViÃ¡vel

### Modelos Locais Recomendados (Ollama)

| Modelo | Tamanho | RAM | Velocidade | RecomendaÃ§Ã£o |
|--------|---------|-----|------------|--------------|
| **phi3:mini** | 2.3GB | 4GB | RÃ¡pido | â­â­â­â­â­ MELHOR OPÃ‡ÃƒO |
| **llama3.2:3b** | 2GB | 4GB | RÃ¡pido | â­â­â­â­â­ Ã“TIMO |
| **gemma2:2b** | 1.6GB | 3GB | Muito RÃ¡pido | â­â­â­â­ BOM |
| **mistral:7b-q4** | 4.1GB | 6GB | MÃ©dio | â­â­â­ OK |
| **llama3.1:8b-q4** | 4.7GB | 8GB | Lento | â­â­ Limite Superior |

### âŒ NÃƒO Recomendado

- Modelos > 8B parÃ¢metros (ficarÃ¡ muito lento)
- Modelos sem quantizaÃ§Ã£o
- Usar GPU AMD (ROCm Ã© complexo e nÃ£o vale o esforÃ§o)
- Modelos FP16/FP32 (use Q4 ou Q5)

## ğŸš€ Plano de ImplementaÃ§Ã£o Recomendado

### **Fase 1: ExperimentaÃ§Ã£o (Atual) - 100% ViÃ¡vel**

**Objetivo**: Testar diferentes LLMs e escolher o melhor

**Setup**:
```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Baixar modelos LEVES (ordem de prioridade)
ollama pull phi3:mini        # 2.3GB - Prioridade 1
ollama pull llama3.2:3b      # 2GB - Prioridade 2
ollama pull gemma2:2b        # 1.6GB - Prioridade 3

# 3. Testar
python src/experiments/run_experiments.py --mode quick
```

**O que vocÃª consegue fazer**:
- âœ… Comparar OpenAI vs modelos locais pequenos
- âœ… Avaliar qualidade vs custo
- âœ… Benchmark com 3-5 modelos simultaneamente
- âœ… MLflow tracking de todos os experimentos

**Tempo estimado**: 1-2 horas de setup + experimentaÃ§Ã£o

**Custo**: $0 (modelos locais) + ~$0.10 (testes OpenAI)

---

### **Fase 2: Fine-Tuning (Futuro) - Parcialmente ViÃ¡vel**

**RestriÃ§Ãµes do seu hardware**:
- âŒ Fine-tuning completo de 7B+ = NÃƒO VIÃVEL (RAM insuficiente)
- âš ï¸ LoRA/QLoRA de 3B = VIÃVEL mas lento (CPU only)
- âœ… Fine-tuning via cloud = VIÃVEL (Google Colab, AWS, Azure)

**RecomendaÃ§Ãµes**:

#### OpÃ§Ã£o A: Fine-Tuning Local (3B apenas)
```python
# Use Phi-3 Mini (3.8B) com LoRA
# ConfiguraÃ§Ã£o otimizada para sua RAM
from peft import LoraConfig

config = LoraConfig(
    r=8,              # Rank baixo (menos memÃ³ria)
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
)

# Treinar em batch pequeno
training_args = TrainingArguments(
    per_device_train_batch_size=1,  # Batch size mÃ­nimo
    gradient_accumulation_steps=4,   # Simular batch maior
    fp16=False,                      # CPU nÃ£o tem FP16
    max_steps=100,                   # Poucas iteraÃ§Ãµes
)
```

**Tempo**: 4-8 horas para 100 steps (muito lento)
**Viabilidade**: 3/10 - PossÃ­vel mas nÃ£o prÃ¡tico

#### OpÃ§Ã£o B: Fine-Tuning em Cloud (RECOMENDADO)
```bash
# Google Colab (Gratuito com GPU T4)
# - 15 GB RAM
# - GPU NVIDIA T4
# - Pode treinar modelos 7B com LoRA

# Ou AWS SageMaker / Azure ML
# - Paga por hora de uso
# - GPU potentes
```

**Tempo**: 30 min - 2 horas
**Custo**: $0 (Colab) ou $1-5/hora (Cloud)
**Viabilidade**: 10/10 - Totalmente prÃ¡tico

---

### **Fase 3: ProduÃ§Ã£o - HÃ­brido Recomendado**

**Arquitetura Recomendada**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Sua MÃ¡quina (Local)             â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Phi-3 Mini  â”‚  â”‚   Ollama API    â”‚ â”‚
â”‚  â”‚   (Preview)  â”‚  â”‚  localhost:11434â”‚ â”‚
â”‚  â”‚   2.3 GB     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚         â†“ (Se aprovado)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  OpenAI API  â”‚ â† Resposta Final    â”‚
â”‚  â”‚ (ProduÃ§Ã£o)   â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Vantagens**:
- âœ… Preview rÃ¡pido e gratuito (Phi-3)
- âœ… Qualidade alta no resultado final (OpenAI)
- âœ… Custo otimizado (paga sÃ³ respostas aprovadas)
- âœ… NÃ£o precisa de servidor potente

**ImplementaÃ§Ã£o**:
```python
# src/agent/hybrid_llm.py
class HybridLLM:
    def __init__(self):
        self.preview_llm = MultiLLMManager.create_llm("ollama", "phi3:mini")
        self.production_llm = MultiLLMManager.create_llm("openai", "gpt-4o-mini")
    
    def generate(self, question, use_preview=True):
        if use_preview:
            # Preview rÃ¡pido (grÃ¡tis)
            preview = self.preview_llm.invoke(question)
            # UsuÃ¡rio pode aprovar ou regenerar
            return preview
        else:
            # ProduÃ§Ã£o (pago, melhor qualidade)
            return self.production_llm.invoke(question)
```

---

## ğŸ’¡ RecomendaÃ§Ã£o Final por Caso de Uso

### 1. **Desenvolvimento e Testes** (Agora)
```bash
# Setup mÃ­nimo
ollama pull phi3:mini
python src/experiments/run_experiments.py --mode quick
```
**Viabilidade**: â­â­â­â­â­ (100%)

### 2. **ExperimentaÃ§Ã£o e Benchmark** (Semana 1-2)
```bash
# Testar 3-4 modelos locais
ollama pull phi3:mini
ollama pull llama3.2:3b
ollama pull gemma2:2b

# Comparar com OpenAI
python src/experiments/run_experiments.py --mode full
```
**Viabilidade**: â­â­â­â­â­ (100%)
**Custo**: ~$0.50 (OpenAI) + $0 (locais)

### 3. **Fine-Tuning de Modelos** (Futuro)
```bash
# Use Google Colab (gratuito)
# Notebook: notebooks/fine_tuning_colab.ipynb
# - Upload dataset
# - Fine-tune Llama 3.2 3B ou Mistral 7B
# - Download modelo
# - Carregar no Ollama local
```
**Viabilidade**: â­â­â­â­ (80% - depende de cloud)
**Custo**: $0 (Colab) ou $5-20 (se usar cloud pago)

### 4. **ProduÃ§Ã£o** (Quando estÃ¡vel)

**OpÃ§Ã£o A: HÃ­brido (RECOMENDADO para vocÃª)**
- Local: Preview/desenvolvimento (Phi-3)
- Cloud: ProduÃ§Ã£o/respostas finais (OpenAI)
- **Viabilidade**: â­â­â­â­â­ (100%)
- **Custo**: ~$10-50/mÃªs (depende do volume)

**OpÃ§Ã£o B: 100% Local**
- Apenas modelos pequenos (Phi-3, Llama 3.2 3B)
- Qualidade inferior mas zero custo
- **Viabilidade**: â­â­â­ (60% - qualidade limitada)
- **Custo**: $0

**OpÃ§Ã£o C: 100% Cloud**
- OpenAI para tudo
- Melhor qualidade
- **Viabilidade**: â­â­â­â­â­ (100%)
- **Custo**: $50-200/mÃªs (depende do volume)

---

## ğŸ¯ PrÃ³ximos Passos PrÃ¡ticos (Ordem Recomendada)

### Semana 1: Setup e ExperimentaÃ§Ã£o
```bash
# Dia 1-2: Setup bÃ¡sico
1. Instalar Ollama
2. Baixar phi3:mini (mais leve e rÃ¡pido)
3. Testar manualmente

# Dia 3-4: Benchmark
4. Executar run_experiments.py --mode quick
5. Comparar Phi-3 vs OpenAI
6. Analisar resultados no MLflow

# Dia 5: DecisÃ£o
7. Escolher: modelo local OU OpenAI OU hÃ­brido
8. Documentar decisÃ£o e mÃ©tricas
```

### Semana 2: OtimizaÃ§Ã£o
```bash
# Se escolheu local:
1. Testar llama3.2:3b e gemma2:2b
2. Otimizar prompts para modelo escolhido
3. Configurar cache de respostas

# Se escolheu hÃ­brido:
4. Implementar sistema de preview+produÃ§Ã£o
5. Definir regras de quando usar cada um

# Se escolheu OpenAI:
6. Otimizar custos (cache, batch)
7. Monitorar usage e custos
```

### MÃªs 1-2: Curadoria de Dados
```bash
# Preparar para fine-tuning futuro
1. Coletar perguntas reais de usuÃ¡rios
2. Revisar e melhorar respostas
3. Criar dataset de 500-1000 pares Q&A
4. Validar qualidade dos dados
```

### MÃªs 2-3: Fine-Tuning (Opcional)
```bash
# Se decidir fine-tunar:
1. Setup Google Colab com GPU
2. Fine-tune Llama 3.2 3B ou Phi-3
3. Avaliar modelo customizado
4. Comparar com base model
5. Deploy se melhorar mÃ©tricas
```

---

## ğŸ“ˆ Benchmark Esperado no Seu Hardware

Com base nas especificaÃ§Ãµes:

| Modelo | LatÃªncia | Qualidade | Custo | RecomendaÃ§Ã£o |
|--------|----------|-----------|-------|--------------|
| Phi-3 Mini | ~2-3s | 7/10 | $0 | â­â­â­â­â­ Use para dev |
| Llama 3.2 3B | ~2-4s | 7.5/10 | $0 | â­â­â­â­ Alternativa |
| Gemma 2B | ~1-2s | 6.5/10 | $0 | â­â­â­ RÃ¡pido mas bÃ¡sico |
| Mistral 7B-Q4 | ~5-8s | 8/10 | $0 | â­â­ Lento demais |
| GPT-4o-mini | ~1s | 9.5/10 | $0.0001/resp | â­â­â­â­â­ ProduÃ§Ã£o |

**Veredicto**: 
- **Desenvolvimento**: Phi-3 Mini local
- **ProduÃ§Ã£o**: GPT-4o-mini (OpenAI) ou hÃ­brido
- **Fine-tuning**: Google Colab com GPU

---

## ğŸ”§ Comandos Otimizados para Seu Sistema

### Setup Inicial Otimizado
```bash
# 1. Instalar apenas o essencial
pip install mlflow langchain-ollama transformers --no-cache-dir

# 2. Ollama com modelo leve
ollama pull phi3:mini

# 3. Configurar para usar CPU eficientemente
export OLLAMA_NUM_PARALLEL=2  # Apenas 2 requests paralelos
export OLLAMA_MAX_LOADED_MODELS=1  # Apenas 1 modelo na RAM
```

### Testar Performance
```bash
# Benchmark apenas modelos viÃ¡veis
python << EOF
from src.experiments.multi_llm import MultiLLMManager
import time

# Teste de latÃªncia
llm = MultiLLMManager.create_llm("ollama", "phi3:mini")

start = time.time()
response = llm.invoke("O que Ã© Retail Media?")
latency = time.time() - start

print(f"LatÃªncia: {latency:.2f}s")
print(f"Resposta: {response[:100]}...")
EOF
```

---

## ğŸ’° AnÃ¡lise de Custos

### CenÃ¡rio 1: 100% Local (Phi-3)
- Setup: $0
- Mensal: $0
- Energia: ~$2-5/mÃªs
- **Total**: $2-5/mÃªs

### CenÃ¡rio 2: HÃ­brido (Phi-3 + OpenAI)
- Setup: $0
- Preview (local): $0
- ProduÃ§Ã£o (1000 resp/mÃªs): ~$15
- **Total**: $15-20/mÃªs

### CenÃ¡rio 3: 100% OpenAI
- Setup: $0
- 1000 respostas/mÃªs: ~$15
- 5000 respostas/mÃªs: ~$75
- **Total**: $15-75/mÃªs

**RecomendaÃ§Ã£o**: CenÃ¡rio 2 (HÃ­brido) - melhor custo-benefÃ­cio

---

## âœ… Checklist de Viabilidade

- âœ… ExperimentaÃ§Ã£o com 3-4 LLMs locais
- âœ… Benchmark automatizado
- âœ… MLflow tracking
- âœ… Jupyter notebooks interativos
- âœ… ComparaÃ§Ã£o com OpenAI
- âš ï¸ Fine-tuning local (3B apenas, lento)
- âœ… Fine-tuning em cloud (Colab/AWS)
- âœ… Deploy hÃ­brido (local + cloud)
- âŒ Modelos grandes (13B+)
- âŒ GPU training (sem CUDA)

**Score de Viabilidade**: 8/10 â­â­â­â­

VocÃª pode fazer **quase tudo**, exceto rodar modelos grandes localmente. A soluÃ§Ã£o hÃ­brida Ã© perfeita para suas necessidades!

