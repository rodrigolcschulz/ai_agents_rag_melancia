# ğŸ¯ Guia Completo de Fine-Tuning com QLoRA

Este guia ensina como fazer fine-tuning de LLMs usando QLoRA no Google Colab (GPU gratuita).

## ğŸ“‹ Ãndice

1. [O que Ã© QLoRA?](#o-que-Ã©-qlora)
2. [Quando fazer fine-tuning?](#quando-fazer-fine-tuning)
3. [PreparaÃ§Ã£o de dados](#preparaÃ§Ã£o-de-dados)
4. [Fine-tuning no Colab](#fine-tuning-no-colab)
5. [Usando o modelo treinado](#usando-o-modelo-treinado)
6. [Troubleshooting](#troubleshooting)
7. [Melhores prÃ¡ticas](#melhores-prÃ¡ticas)

---

## O que Ã© QLoRA?

**QLoRA (Quantized Low-Rank Adaptation)** Ã© uma tÃ©cnica eficiente de fine-tuning que permite treinar modelos LLM grandes com memÃ³ria GPU limitada.

### Como funciona:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Modelo Base (7B params)                 â”‚
â”‚  Quantizado em 4-bit (~4 GB)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  + LoRA Adapters (~50-200 MB)            â”‚
â”‚  Apenas 0.1-1% dos parÃ¢metros treinÃ¡veis â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Modelo Fine-Tunado                      â”‚
â”‚  Especializado no seu domÃ­nio            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vantagens:

- âœ… **Eficiente**: Treina em GPU T4 gratuita (Colab)
- âœ… **RÃ¡pido**: 30 min - 2 horas de treino
- âœ… **Leve**: Apenas adaptadores (~50-200 MB) vs modelo completo (4-14 GB)
- âœ… **Qualidade**: Resultados comparÃ¡veis a fine-tuning completo

### Paper Original:

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) (Dettmers et al., 2023)

---

## Quando fazer fine-tuning?

### âœ… FaÃ§a fine-tuning quando:

1. **DomÃ­nio especÃ­fico**: Seu caso usa terminologia/conceitos Ãºnicos
   - Ex: Retail Media, jurÃ­dico, mÃ©dico, tÃ©cnico

2. **Estilo especÃ­fico**: Precisa de tom/formato particular
   - Ex: Formal, casual, tÃ©cnico, didÃ¡tico

3. **Performance insuficiente**: Modelos base nÃ£o atendem qualidade
   - Respostas genÃ©ricas, imprecisas, ou irrelevantes

4. **Dados proprietÃ¡rios**: VocÃª tem dados Ãºnicos valiosos
   - Logs de interaÃ§Ãµes, documentaÃ§Ã£o interna, Q&A curado

5. **Custo**: Quer reduzir custos de API
   - Substituir GPT-4 por modelo local customizado

### âŒ NÃƒO faÃ§a fine-tuning quando:

1. **Falta de dados**: Menos de 50-100 exemplos de qualidade
2. **RAG resolve**: Problema Ã© falta de contexto (use RAG primeiro)
3. **Prompts funcionam**: Engenharia de prompt jÃ¡ resolve
4. **Tempo/recursos limitados**: Fine-tuning requer iteraÃ§Ã£o

### ğŸ¯ Abordagem Recomendada:

```
1. Prompt Engineering
   â†“ (nÃ£o resolveu?)
2. RAG (Retrieval)
   â†“ (ainda insuficiente?)
3. Fine-Tuning
```

---

## PreparaÃ§Ã£o de Dados

### 1. Coleta de Dados

VocÃª precisa de **pares pergunta-resposta** de qualidade.

#### Fontes de dados:

- **Logs do sistema RAG**: InteraÃ§Ãµes reais dos usuÃ¡rios
- **DocumentaÃ§Ã£o**: Convertida em Q&A
- **FAQs**: Perguntas frequentes curadas
- **Especialistas**: Respostas revisadas por humanos

#### Quantidade mÃ­nima:

| Dataset | MÃ­nimo | Recomendado | Ideal |
|---------|--------|-------------|-------|
| **Testes** | 10-20 | 50 | 100 |
| **ProduÃ§Ã£o** | 100-200 | 500-1000 | 2000+ |

### 2. Formato dos Dados

QLoRA usa templates especÃ­ficos. Os mais comuns:

#### Formato Alpaca (Recomendado):

```text
### Instruction:
O que Ã© Retail Media?

### Response:
Retail Media Ã© uma forma de publicidade digital onde varejistas monetizam...
```

#### Formato ChatML (Phi-3, GPT):

```text
<|system|>
VocÃª Ã© um assistente especializado em Retail Media.
<|end|>
<|user|>
O que Ã© Retail Media?
<|end|>
<|assistant|>
Retail Media Ã© uma forma de publicidade...
<|end|>
```

#### Formato Llama 2:

```text
[INST] <<SYS>>
VocÃª Ã© um assistente especializado em Retail Media.
<</SYS>>

O que Ã© Retail Media? [/INST] Retail Media Ã©...
```

### 3. Preparar Dataset (Local)

Use o notebook: `notebooks/prepare_finetuning_data.ipynb`

```python
from src.finetuning.data_prep import DatasetPreparator

# Criar preparator
preparator = DatasetPreparator(template="alpaca")

# OpÃ§Ã£o 1: De JSON
dataset = preparator.prepare_from_json("data.json")

# OpÃ§Ã£o 2: De CSV
dataset = preparator.prepare_from_csv("data.csv")

# OpÃ§Ã£o 3: De logs RAG
dataset = preparator.prepare_from_rag_logs("logs/", min_quality_score=4.0)

# Dividir treino/validaÃ§Ã£o
split_dataset = preparator.create_train_test_split(dataset, test_size=0.1)

# Salvar
preparator.save_dataset(split_dataset, "data/finetuning/processed/my_dataset")
```

### 4. Qualidade dos Dados

**CRITICAL**: Qualidade > Quantidade

âœ… **Boas prÃ¡ticas:**

- âœ… Respostas completas e precisas
- âœ… Diversidade de perguntas
- âœ… Cobertura de diferentes tÃ³picos
- âœ… Revisar manualmente alguns exemplos
- âœ… Remover exemplos ruins/ambÃ­guos

âŒ **Evite:**

- âŒ Respostas genÃ©ricas/Ã³bvias
- âŒ Erros factuais
- âŒ ViÃ©s ou linguagem problemÃ¡tica
- âŒ Duplicatas
- âŒ Respostas muito curtas (<50 chars) ou longas (>2000 chars)

---

## Fine-Tuning no Colab

### 1. Setup Google Colab

1. **Acessar**: [Google Colab](https://colab.research.google.com/)
2. **Ativar GPU**: 
   - Runtime â†’ Change runtime type
   - Hardware accelerator â†’ **T4 GPU**
3. **Upload notebook**: `notebooks/fine_tuning_qlora_colab.ipynb`

### 2. Fazer Upload do Dataset

**OpÃ§Ã£o A: Google Drive (Recomendado)**

```python
# No Colab
from google.colab import drive
drive.mount('/content/drive')

# Dataset em: /content/drive/MyDrive/training_dataset
```

**OpÃ§Ã£o B: Upload direto**

```python
from google.colab import files
uploaded = files.upload()
```

### 3. Escolher Modelo Base

Modelos recomendados para Colab (GPU T4):

| Modelo | Tamanho | Velocidade | Qualidade | RAM GPU |
|--------|---------|------------|-----------|---------|
| **microsoft/phi-2** | 2.7B | âš¡âš¡âš¡ RÃ¡pido | â­â­â­ Boa | ~8 GB |
| **meta-llama/Llama-2-7b-hf** | 7B | âš¡âš¡ MÃ©dio | â­â­â­â­ Ã“tima | ~12 GB |
| **mistralai/Mistral-7B-v0.1** | 7B | âš¡âš¡ MÃ©dio | â­â­â­â­â­ Excelente | ~12 GB |

**RecomendaÃ§Ã£o:** Comece com **Phi-2** para testes rÃ¡pidos, depois use **Mistral 7B** para produÃ§Ã£o.

### 4. Configurar ParÃ¢metros

```python
# No notebook Colab, ajuste:

MODEL_NAME = "microsoft/phi-2"
DATASET_PATH = "/content/drive/MyDrive/training_dataset"
OUTPUT_MODEL_NAME = "phi2-retail-media"

# HiperparÃ¢metros
EPOCHS = 3              # Mais epochs = mais aprendizado (cuidado com overfitting)
BATCH_SIZE = 4          # Reduza se ficar sem memÃ³ria
LEARNING_RATE = 2e-4    # Taxa de aprendizado (geralmente 1e-4 a 5e-4)
MAX_SEQ_LENGTH = 2048   # Tamanho mÃ¡ximo de contexto

# LoRA
LORA_R = 16             # Rank (8-64, maior = mais capacidade)
LORA_ALPHA = 32         # Geralmente 2x o rank
```

### 5. Executar Treinamento

Execute as cÃ©lulas do notebook sequencialmente:

1. âœ… Verificar GPU
2. âœ… Instalar dependÃªncias
3. âœ… Carregar dataset
4. âœ… Carregar modelo com quantizaÃ§Ã£o
5. âœ… Aplicar LoRA
6. âœ… Tokenizar dataset
7. âœ… **Treinar** â±ï¸ (30 min - 2 horas)
8. âœ… Salvar modelo
9. âœ… Testar

### 6. Monitorar Treinamento

Durante o treino, observe:

- **Loss**: Deve diminuir gradualmente
  - Inicial: ~2-3
  - Final: ~0.5-1.5
  - Se nÃ£o diminui: aumente learning rate
  - Se diminui muito rÃ¡pido: reduza learning rate

- **GPU Memory**: 
  - Uso: ~80-90% Ã© ideal
  - Se 100%: reduza BATCH_SIZE ou MAX_SEQ_LENGTH
  - Se <50%: pode aumentar BATCH_SIZE

- **Tempo por step**:
  - Phi-2: ~1-2s/step
  - Llama/Mistral 7B: ~3-5s/step

### 7. Salvar e Download

Modelo salvo automaticamente em:
- **Local**: `/content/models/{OUTPUT_MODEL_NAME}/`
- **Google Drive**: `/content/drive/MyDrive/finetuned_models/{OUTPUT_MODEL_NAME}/`
- **ZIP**: Para download direto

---

## Usando o Modelo Treinado

### OpÃ§Ã£o 1: Testar no Colab

JÃ¡ incluÃ­do no notebook:

```python
def generate_response(prompt):
    formatted = f"### Instruction:\n{prompt}\n\n### Response:\n"
    inputs = tokenizer(formatted, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Testar
response = generate_response("O que Ã© Retail Media?")
print(response)
```

### OpÃ§Ã£o 2: Usar Localmente com Transformers

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# Carregar modelo base
base_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Carregar adaptadores LoRA
model = PeftModel.from_pretrained(base_model, "./phi2-retail-media")

# Carregar tokenizer
tokenizer = AutoTokenizer.from_pretrained("./phi2-retail-media")

# Usar
def ask(question):
    prompt = f"### Instruction:\n{question}\n\n### Response:\n"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=512)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

response = ask("O que Ã© programmatic advertising?")
print(response)
```

### OpÃ§Ã£o 3: Converter para Ollama (Recomendado)

Para usar localmente sem GPU potente:

#### Passo 1: Merge LoRA com modelo base

```python
from src.finetuning.export_to_ollama import ModelExporter

exporter = ModelExporter("./phi2-retail-media")
merged_path = exporter.merge_lora_adapters(
    base_model_name="microsoft/phi-2",
    output_path="./phi2-retail-media-merged"
)
```

#### Passo 2: Converter para GGUF (opcional)

Requer [llama.cpp](https://github.com/ggerganov/llama.cpp) instalado:

```bash
# Instalar llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# Converter
python convert.py /path/to/phi2-retail-media-merged --outfile phi2-retail.fp16.gguf
./quantize phi2-retail.fp16.gguf phi2-retail.q4_k_m.gguf q4_k_m
```

#### Passo 3: Criar Modelfile Ollama

```python
from src.finetuning.export_to_ollama import ModelExporter

exporter.create_ollama_modelfile(
    gguf_path="./phi2-retail.q4_k_m.gguf",
    output_path="./Modelfile",
    model_name="phi2-retail"
)
```

#### Passo 4: Usar com Ollama

```bash
# Criar modelo no Ollama
ollama create phi2-retail -f Modelfile

# Usar
ollama run phi2-retail "O que Ã© Retail Media?"
```

#### Passo 5: Integrar no seu cÃ³digo

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(model="phi2-retail", temperature=0.7)
response = llm.invoke("O que Ã© programmatic advertising?")
print(response.content)
```

---

## Troubleshooting

### Problema: Out of Memory (OOM)

**Sintoma**: `CUDA out of memory` durante treinamento

**SoluÃ§Ãµes**:

1. **Reduzir batch size**:
   ```python
   BATCH_SIZE = 2  # ou 1
   GRADIENT_ACCUMULATION = 8  # compensar batch menor
   ```

2. **Reduzir sequence length**:
   ```python
   MAX_SEQ_LENGTH = 1024  # ao invÃ©s de 2048
   ```

3. **Reduzir LoRA rank**:
   ```python
   LORA_R = 8  # ao invÃ©s de 16
   ```

4. **Usar modelo menor**:
   ```python
   MODEL_NAME = "microsoft/phi-2"  # ao invÃ©s de Llama 7B
   ```

### Problema: Loss nÃ£o diminui

**Sintoma**: Loss fica estÃ¡vel ou aumenta

**SoluÃ§Ãµes**:

1. **Aumentar learning rate**:
   ```python
   LEARNING_RATE = 5e-4  # ao invÃ©s de 2e-4
   ```

2. **Verificar dados**:
   - Dataset estÃ¡ no formato correto?
   - Exemplos fazem sentido?

3. **Treinar mais**:
   ```python
   EPOCHS = 5  # ao invÃ©s de 3
   ```

### Problema: Overfitting

**Sintoma**: Loss de treino baixa, mas modelo nÃ£o generaliza

**SoluÃ§Ãµes**:

1. **Mais dados**: Adicionar mais exemplos diversos

2. **Menos epochs**:
   ```python
   EPOCHS = 2
   ```

3. **Aumentar dropout**:
   ```python
   LORA_DROPOUT = 0.1  # ao invÃ©s de 0.05
   ```

4. **ValidaÃ§Ã£o regular**: Sempre use split treino/validaÃ§Ã£o

### Problema: Modelo nÃ£o segue instruÃ§Ãµes

**Sintoma**: Respostas nÃ£o fazem sentido ou ignoram pergunta

**SoluÃ§Ãµes**:

1. **Verificar template**: Usar formato correto (Alpaca, ChatML, etc)

2. **Exemplos de qualidade**: Revisar se respostas no dataset sÃ£o boas

3. **Mais dados**: Adicionar mais exemplos variados

4. **Testar modelo base**: Verificar se modelo base jÃ¡ segue instruÃ§Ãµes

### Problema: Colab desconecta

**Sintoma**: SessÃ£o Colab desconecta durante treino longo

**SoluÃ§Ãµes**:

1. **Salvar checkpoints**:
   ```python
   save_steps = 50  # salva a cada 50 steps
   ```

2. **Backup no Drive**: JÃ¡ configurado no notebook

3. **Manter aba ativa**: Deixar aba do Colab aberta

4. **Colab Pro**: Considerar upgrade ($10/mÃªs)

---

## Melhores PrÃ¡ticas

### 1. Curadoria de Dados

- âœ… **Qualidade primeiro**: 100 exemplos excelentes > 1000 medianos
- âœ… **Diversidade**: Cubra diferentes tipos de perguntas
- âœ… **RevisÃ£o humana**: Sempre revisar alguns exemplos
- âœ… **Limpeza**: Remover duplicatas, erros, exemplos ruins
- âœ… **ValidaÃ§Ã£o**: Sempre usar split treino/validaÃ§Ã£o (90/10)

### 2. Escolha de Modelo

| Use Case | Modelo Recomendado | Justificativa |
|----------|-------------------|---------------|
| **Testes rÃ¡pidos** | Phi-2 (2.7B) | RÃ¡pido, roda em qualquer GPU |
| **ProduÃ§Ã£o balanceada** | Mistral 7B | Melhor custo-benefÃ­cio |
| **MÃ¡xima qualidade** | Llama 2 7B | Excelente, bem suportado |
| **Muito especÃ­fico** | Qualquer + mais dados | Dataset Ã© mais importante |

### 3. HiperparÃ¢metros

**ConfiguraÃ§Ã£o conservadora (recomendada)**:

```python
EPOCHS = 3
BATCH_SIZE = 4
GRADIENT_ACCUMULATION = 4
LEARNING_RATE = 2e-4
MAX_SEQ_LENGTH = 2048
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
```

**Para dataset pequeno (<200 exemplos)**:

```python
EPOCHS = 5  # Treinar mais
LORA_R = 8  # Rank menor (evitar overfit)
```

**Para dataset grande (>1000 exemplos)**:

```python
EPOCHS = 2  # Menos epochs
LORA_R = 32  # Rank maior (mais capacidade)
```

### 4. AvaliaÃ§Ã£o

Sempre avaliar em mÃºltiplas dimensÃµes:

1. **Loss quantitativo**: Training/validation loss
2. **Testes qualitativos**: Perguntas especÃ­ficas do domÃ­nio
3. **ComparaÃ§Ã£o**: vs modelo base, vs OpenAI
4. **UsuÃ¡rios reais**: Beta testing com usuÃ¡rios

### 5. IteraÃ§Ã£o

Fine-tuning Ã© um processo iterativo:

```
1. Baseline (modelo base)
   â†“
2. Fine-tune v1 (dataset inicial)
   â†“
3. Avaliar e coletar feedback
   â†“
4. Melhorar dataset
   â†“
5. Fine-tune v2
   â†“
6. Repetir atÃ© satisfatÃ³rio
```

### 6. Deployment

**OpÃ§Ã£o A: Local (CPU)**
- âœ… Zero custo
- âœ… Privacidade total
- âŒ LatÃªncia alta (2-5s)
- **Ideal para**: Desenvolvimento, baixo volume

**OpÃ§Ã£o B: Local (GPU)**
- âœ… LatÃªncia baixa (<1s)
- âœ… Privacidade
- âŒ Requer GPU NVIDIA
- **Ideal para**: ProduÃ§Ã£o, alto volume

**OpÃ§Ã£o C: Cloud (RunPod, AWS)**
- âœ… EscalÃ¡vel
- âœ… GPU potentes
- âŒ Custo mensal
- **Ideal para**: ProduÃ§Ã£o, muitos usuÃ¡rios

**OpÃ§Ã£o D: HÃ­brido**
- Modelo local para preview
- OpenAI para respostas finais
- **Ideal para**: Otimizar custos

---

## Recursos Adicionais

### Papers

- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

### DocumentaÃ§Ã£o

- [PEFT (HuggingFace)](https://huggingface.co/docs/peft)
- [Transformers](https://huggingface.co/docs/transformers)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)
- [TRL](https://huggingface.co/docs/trl)

### Ferramentas

- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) - Framework de fine-tuning
- [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) - UI para fine-tuning
- [Unsloth](https://github.com/unslothai/unsloth) - Fine-tuning otimizado

### Comunidades

- [HuggingFace Discord](https://hf.co/join/discord)
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)
- [r/MachineLearning](https://reddit.com/r/MachineLearning)

---

## FAQ

### Quanto custa fazer fine-tuning?

- **Google Colab (gratuito)**: $0
- **Google Colab Pro**: $10/mÃªs (GPU melhor, mais tempo)
- **RunPod**: $0.30-0.80/hora (pay-as-you-go)
- **AWS SageMaker**: $1-5/hora

Para 1 fine-tuning: ~$0-2

### Quanto tempo leva?

- **Phi-2 (2.7B)**: 20-40 min (100 exemplos, 3 epochs)
- **Llama/Mistral 7B**: 1-2 horas (100 exemplos, 3 epochs)

### Quantos dados preciso?

- **MÃ­nimo viÃ¡vel**: 50-100 exemplos
- **Recomendado**: 500-1000 exemplos
- **Ideal**: 2000+ exemplos

### Posso fine-tunar GPT-4?

NÃ£o. GPT-4 Ã© closed-source. Mas vocÃª pode:
- Fine-tunar GPT-3.5 Turbo via OpenAI API
- Fine-tunar modelos open-source (Llama, Mistral, Phi)

### Fine-tuning Ã© melhor que RAG?

**NÃ£o Ã© "ou"**, Ã© **"e"**:

- **RAG**: Adiciona conhecimento/contexto
- **Fine-tuning**: Melhora estilo/comportamento

**Melhor**: RAG + Fine-tuning juntos

### Posso comercializar modelo fine-tunado?

Depende da licenÃ§a do modelo base:

- âœ… **Llama 2**: Sim (comercial atÃ© 700M users)
- âœ… **Mistral**: Sim (Apache 2.0)
- âœ… **Phi-2**: Sim (MIT license)
- âŒ **Llama 1**: NÃ£o (apenas pesquisa)

Sempre verifique a licenÃ§a!

---

## PrÃ³ximos Passos

1. âœ… Preparar seu dataset (`prepare_finetuning_data.ipynb`)
2. âœ… Fazer fine-tuning no Colab (`fine_tuning_qlora_colab.ipynb`)
3. âœ… Testar e avaliar modelo
4. âœ… Iterar e melhorar
5. âœ… Deploy em produÃ§Ã£o

**Boa sorte com seu fine-tuning! ğŸš€**

---

**DÃºvidas?** Abra uma issue no repositÃ³rio ou consulte a documentaÃ§Ã£o.

