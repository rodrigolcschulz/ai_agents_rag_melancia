# ğŸš€ Guia de Benchmark de Modelos LLM

## ğŸ¯ Modelos Recomendados para Seu Hardware

Baseado no seu AMD Ryzen 5 3600X com 15GB RAM (CPU only):

### âš¡ Categoria: Ultra RÃ¡pidos (<3s resposta)

| Modelo | Tamanho | RAM | DescriÃ§Ã£o | Install |
|--------|---------|-----|-----------|---------|
| **gemma2:2b** | 1.6GB | 3GB | Google, ultra rÃ¡pido e eficiente | `ollama pull gemma2:2b` |
| **qwen2.5:3b** | 2GB | 4GB | Alibaba, multilÃ­ngue, Ã³timo | `ollama pull qwen2.5:3b` |
| **phi3:mini** | 2.3GB | 4GB | Microsoft, otimizado para CPU | `ollama pull phi3:mini` |

**Melhor escolha**: `qwen2.5:3b` ou `phi3:mini`

---

### ğŸ¯ Categoria: RÃ¡pidos e com Boa Qualidade (3-5s)

| Modelo | Tamanho | RAM | DescriÃ§Ã£o | Install |
|--------|---------|-----|-----------|---------|
| **llama3.2:3b** | 2GB | 4GB | Meta, excelente qualidade/velocidade | `ollama pull llama3.2:3b` |
| **phi3:medium** | 7.9GB | 8GB | Microsoft, melhor contexto | `ollama pull phi3:medium` |
| **gemma2:9b** | 5.5GB | 8GB | Google, alta qualidade | `ollama pull gemma2:9b` |

**Melhor escolha**: `llama3.2:3b` (equilÃ­brio perfeito)

---

### ğŸ† Categoria: Qualidade MÃ¡xima (5-10s)

| Modelo | Tamanho | RAM | DescriÃ§Ã£o | Install |
|--------|---------|-----|-----------|---------|
| **mistral:7b-instruct-q4_K_M** | 4.1GB | 6GB | Mistral AI, excelente raciocÃ­nio | `ollama pull mistral:7b-instruct-q4_K_M` |
| **llama3.1:8b-instruct-q4_K_M** | 4.7GB | 8GB | Meta, versÃ£o avanÃ§ada | `ollama pull llama3.1:8b-instruct-q4_K_M` |

**Melhor escolha**: `mistral:7b-instruct-q4_K_M` (melhor custo-benefÃ­cio)

---

## ğŸ†• Modelos Novos Recomendados (NÃ£o estavam no doc original)

### 1. **Qwen 2.5 3B** â­â­â­â­â­
```bash
ollama pull qwen2.5:3b
```
- **Por quÃª**: Alibaba lanÃ§ou recentemente, multilÃ­ngue excelente
- **Vantagens**: RÃ¡pido, contexto longo (32k tokens), Ã³timo em portuguÃªs
- **LatÃªncia esperada**: 2-4s
- **Qualidade**: 8/10

### 2. **Phi-3 Medium** â­â­â­â­
```bash
ollama pull phi3:medium
```
- **Por quÃª**: VersÃ£o maior do Phi-3, mantÃ©m eficiÃªncia
- **Vantagens**: Melhor raciocÃ­nio que o mini, ainda rÃ¡pido
- **LatÃªncia esperada**: 4-6s
- **Qualidade**: 8.5/10

### 3. **Gemma 2 9B** â­â­â­â­
```bash
ollama pull gemma2:9b
```
- **Por quÃª**: Google, qualidade prÃ³xima a modelos maiores
- **Vantagens**: Excelente em tarefas especÃ­ficas, bem treinado
- **LatÃªncia esperada**: 5-8s
- **Qualidade**: 8.5/10

---

## ğŸ“Š Como Executar o Benchmark

### Passo 1: Instalar Modelos Recomendados

```bash
# Modelos essenciais (rÃ¡pido, ~10 min)
ollama pull gemma2:2b
ollama pull qwen2.5:3b
ollama pull phi3:mini
ollama pull llama3.2:3b

# Modelos adicionais de qualidade (opcional, ~15 min)
ollama pull mistral:7b-instruct-q4_K_M
ollama pull gemma2:9b
```

**Tempo total de download**: 15-30 minutos (depende da conexÃ£o)

---

### Passo 2: Ver Guia de InstalaÃ§Ã£o

```bash
cd /home/coneta/ai_agents_rag_melancia
python src/experiments/benchmark_models_mlflow.py --install-guide
```

**Output:**
```
ğŸ“¥ MODELOS RECOMENDADOS PARA SEU HARDWARE
======================================================================

1. gemma2:2b
   Tamanho: 1.6GB
   Ultra rÃ¡pido, boa qualidade

2. phi3:mini
   Tamanho: 2.3GB
   Microsoft, otimizado para CPU

3. llama3.2:3b
   Tamanho: 2GB
   Meta, excelente qualidade
...
```

---

### Passo 3: Teste RÃ¡pido (Verificar se funciona)

```bash
# Testar modelo especÃ­fico
python src/experiments/benchmark_models_mlflow.py --test-model ollama::phi3:mini
```

**Output esperado:**
```
ğŸ§ª Teste rÃ¡pido: ollama::phi3:mini
----------------------------------------------------------------------
Pergunta: O que Ã© ACOS no contexto de Product Ads?

Resposta (2.34s):
----------------------------------------------------------------------
ACOS (Advertising Cost of Sale) Ã© uma mÃ©trica que mede...
----------------------------------------------------------------------

âœ… Modelo funcional! LatÃªncia: 2.34s
```

---

### Passo 4: Benchmark Completo

#### OpÃ§Ã£o A: Modo QUICK (3-5 minutos)
```bash
# Testa apenas modelos ultra rÃ¡pidos + OpenAI
python src/experiments/benchmark_models_mlflow.py --mode quick
```

#### OpÃ§Ã£o B: Modo FAST (10-15 minutos) â­ RECOMENDADO
```bash
# Testa modelos rÃ¡pidos + qualidade + OpenAI
python src/experiments/benchmark_models_mlflow.py --mode fast
```

#### OpÃ§Ã£o C: Modo FULL (30-60 minutos)
```bash
# Testa TODOS os modelos com todas as perguntas
python src/experiments/benchmark_models_mlflow.py --mode full
```

---

### Passo 5: Visualizar Resultados no MLflow

```bash
# Em um terminal separado
mlflow ui --port 5000

# Abrir navegador em:
# http://localhost:5000
```

**No MLflow vocÃª verÃ¡:**
- âœ… ComparaÃ§Ã£o lado a lado de todos os modelos
- âœ… GrÃ¡ficos de latÃªncia vs qualidade
- âœ… MÃ©tricas detalhadas (P50, P95, P99)
- âœ… Custo por query (OpenAI vs grÃ¡tis)

---

## ğŸ“Š Resultados Esperados

### PrevisÃ£o de Performance no Seu Hardware:

| Modelo | LatÃªncia | Qualidade | Custo/mÃªs | RecomendaÃ§Ã£o |
|--------|----------|-----------|-----------|--------------|
| gemma2:2b | ~1.5s | 6.5/10 | $0 | â­â­â­ ProtÃ³tipos rÃ¡pidos |
| qwen2.5:3b | ~2.5s | 7.5/10 | $0 | â­â­â­â­â­ **MELHOR GERAL** |
| phi3:mini | ~2s | 7/10 | $0 | â­â­â­â­ Ã“timo equilÃ­brio |
| llama3.2:3b | ~3s | 7.8/10 | $0 | â­â­â­â­â­ **ALTA QUALIDADE** |
| mistral:7b-q4 | ~6s | 8.2/10 | $0 | â­â­â­ Qualidade mÃ¡xima |
| gemma2:9b | ~7s | 8.3/10 | $0 | â­â­â­ Alternativa qualidade |
| gpt-4o-mini | ~1.5s | 9.5/10 | ~$15 | â­â­â­â­â­ **BASELINE** |

---

## ğŸ’¡ RecomendaÃ§Ãµes Finais

### Para Desenvolvimento:
```bash
# Use o mais rÃ¡pido para iteraÃ§Ã£o
qwen2.5:3b  # 2.5s, grÃ¡tis, boa qualidade
```

### Para ProduÃ§Ã£o (HÃ­brido):
```bash
# 70% llama3.2:3b (grÃ¡tis, 3s)
# 30% gpt-4o-mini (pago, 1.5s, melhor qualidade)

export OLLAMA_PERCENTAGE=0.7
export OLLAMA_MODEL=llama3.2:3b
```

### Para Qualidade MÃ¡xima:
```bash
# Use 100% OpenAI
export USE_MODEL_ROUTER=false
# Custo: ~$20-50/mÃªs
```

---

## ğŸ¯ PrÃ³ximos Passos

1. **Instalar modelos base**:
   ```bash
   ollama pull qwen2.5:3b
   ollama pull llama3.2:3b
   ollama pull phi3:mini
   ```

2. **Executar benchmark**:
   ```bash
   python src/experiments/benchmark_models_mlflow.py --mode fast
   ```

3. **Analisar resultados no MLflow**:
   ```bash
   mlflow ui --port 5000
   ```

4. **Escolher modelo** baseado em:
   - LatÃªncia aceitÃ¡vel
   - Qualidade necessÃ¡ria
   - OrÃ§amento disponÃ­vel

5. **Atualizar configuraÃ§Ã£o**:
   ```python
   # src/mlops/model_router.py
   DEFAULT_OLLAMA_MODEL = "llama3.2:3b"  # Ou o escolhido
   ```

---

## ğŸ“ Onde Encontrar Resultados

```
data/experiments/
â”œâ”€â”€ model_comparison_results.json  # Resultados em JSON
â””â”€â”€ mlruns/                         # MLflow tracking data
    â””â”€â”€ melancia_model_comparison/  # Experimento especÃ­fico
```

---

## ğŸ› Troubleshooting

### Erro: "ollama: command not found"
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

### Erro: "model not found"
```bash
# Ver modelos instalados
ollama list

# Instalar modelo faltante
ollama pull <modelo>
```

### Erro: "CUDA not available"
âœ… **Normal!** Seu sistema nÃ£o tem GPU NVIDIA. O cÃ³digo usa CPU automaticamente.

### LatÃªncia muito alta (>20s)
- Verificar se outros processos estÃ£o usando CPU
- Considerar modelo menor (gemma2:2b)
- Aumentar % de OpenAI

---

**ğŸ‰ MelancIA** - Pronto para comparar modelos e escolher o melhor para seu projeto!

