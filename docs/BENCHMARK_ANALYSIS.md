# ðŸ” AnÃ¡lise de Resultados do Benchmark

## ðŸ“Š Resultados Obtidos

### âœ… Modelos que Funcionaram

| Modelo | LatÃªncia | Qualidade | RelevÃ¢ncia | Custo/query |
|--------|----------|-----------|------------|-------------|
| **gemma2:2b** | 63.18s ðŸ”´ | 1.00 | 0.50 | $0 |
| **qwen2.5:3b** | 104.67s ðŸ”´ | 1.00 | 0.52 | $0 |
| **phi3:mini** | 266.60s ðŸ”´ðŸ”´ | 1.00 | 0.58 | $0 |
| **llama3.2:3b** | 127.38s ðŸ”´ | 1.00 | 0.63 | $0 |
| **gpt-4o-mini** | 7.89s âœ… | 1.00 | 0.58 | ~$0.000375 |
| **gpt-3.5-turbo** | 5.34s âœ… | 0.80 | 0.42 | ~$0.000125 |

### âŒ Modelos que Falharam (NÃ£o Instalados)

- phi3:medium â†’ `ollama pull phi3:medium`
- gemma2:9b â†’ `ollama pull gemma2:9b`

---

## ðŸš¨ PROBLEMA CRÃTICO: Ollama Extremamente Lento

### LatÃªncias Esperadas vs Reais

| Modelo | Esperado | Real | DiferenÃ§a |
|--------|----------|------|-----------|
| gemma2:2b | ~2s | 63s | **31x mais lento** ðŸ”´ |
| qwen2.5:3b | ~3s | 105s | **35x mais lento** ðŸ”´ |
| phi3:mini | ~2s | 267s | **133x mais lento** ðŸ”´ðŸ”´ |
| llama3.2:3b | ~3s | 127s | **42x mais lento** ðŸ”´ |

### ðŸ” PossÃ­veis Causas

#### 1. **CPU Sobrecarregado**
```bash
# Verificar uso de CPU
htop

# Ver processos do Ollama
ps aux | grep ollama
```

**Sintomas:**
- Outros processos pesados rodando
- CPU usage perto de 100%
- Sistema travando

**SoluÃ§Ã£o:**
- Fechar outros processos
- NÃ£o rodar mÃºltiplos modelos simultaneamente

---

#### 2. **RAM Insuficiente â†’ Usando SWAP**
```bash
# Verificar uso de memÃ³ria
free -h

# Ver swap
swapon --show
```

**Sintomas:**
- Modelos maiores que RAM disponÃ­vel
- Sistema lento em geral
- Disco trabalhando muito (LED piscando)

**Seu hardware:**
- RAM: 15GB total
- gemma2:2b: ~4GB RAM necessÃ¡ria
- qwen2.5:3b: ~6GB RAM necessÃ¡ria
- phi3:mini: ~4GB RAM necessÃ¡ria
- llama3.2:3b: ~5GB RAM necessÃ¡ria

**Se rodar tudo ao mesmo tempo = 19GB > 15GB = SWAP!**

**SoluÃ§Ã£o:**
```bash
# Configurar Ollama para carregar apenas 1 modelo por vez
export OLLAMA_MAX_LOADED_MODELS=1
```

---

#### 3. **Ollama Configurado Incorretamente**
```bash
# Ver configuraÃ§Ã£o atual
ollama ps

# Ver logs
journalctl -u ollama -f
```

**PossÃ­veis problemas:**
- Ollama limitando threads
- ConfiguraÃ§Ã£o de memÃ³ria errada

**SoluÃ§Ã£o:**
```bash
# Configurar threads otimizados para Ryzen 5 3600X (12 threads)
export OLLAMA_NUM_PARALLEL=2  # MÃ¡ximo 2 requests paralelos
export OLLAMA_NUM_THREADS=12  # Usar todos os threads do CPU
```

---

#### 4. **Benchmark Rodando Modelos em Paralelo**

**PROBLEMA**: O cÃ³digo pode estar tentando carregar TODOS os modelos na RAM!

**SoluÃ§Ã£o**: Verificar se o benchmark limpa memÃ³ria entre testes.

---

## âš¡ SoluÃ§Ãµes Imediatas

### 1. Otimizar ConfiguraÃ§Ã£o do Ollama

```bash
# Criar/editar ~/.ollama/config
cat > ~/.ollama/config << 'EOF'
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_NUM_THREADS=12
OLLAMA_NUM_PARALLEL=1
EOF

# Reiniciar Ollama
sudo systemctl restart ollama
# Ou se rodando manualmente:
pkill ollama && ollama serve &
```

### 2. Limpar RAM Entre Testes

```bash
# Antes de executar benchmark
sync && sudo sysctl -w vm.drop_caches=3

# Executar benchmark
python src/experiments/benchmark_models_mlflow.py --mode quick
```

### 3. Testar Modelos Individualmente

```bash
# Testar um por vez
python src/experiments/benchmark_models_mlflow.py --test-model ollama::gemma2:2b

# Se rÃ¡pido (2-3s), problema Ã© memÃ³ria/concorrÃªncia
# Se lento (60s+), problema Ã© CPU/configuraÃ§Ã£o
```

---

## ðŸŽ¯ Modelos Adicionais Recomendados

### Prioridade ALTA (Vale a Pena Instalar)

#### 1. **mistral:7b-instruct-q4_K_M** â­â­â­â­â­
```bash
ollama pull mistral:7b-instruct-q4_K_M
```
**Por quÃª:**
- Qualidade EXCELENTE (melhor que llama3.2:3b)
- RaciocÃ­nio superior
- Bom em portuguÃªs
- ~4.1GB (cabe na RAM)

**LatÃªncia esperada:** 6-10s (se Ollama otimizado)

---

#### 2. **gemma2:9b** â­â­â­â­
```bash
ollama pull gemma2:9b
```
**Por quÃª:**
- Google, qualidade alta
- Melhor que gemma2:2b
- ~5.5GB

**LatÃªncia esperada:** 8-12s

---

### Prioridade MÃ‰DIA (Testes EspecÃ­ficos)

#### 3. **llama3.1:8b-instruct-q4_K_M** â­â­â­
```bash
ollama pull llama3.1:8b-instruct-q4_K_M
```
**Por quÃª:**
- VersÃ£o avanÃ§ada do llama3.2
- Mais capabilities
- ~4.7GB

**LatÃªncia esperada:** 8-15s

---

### Prioridade BAIXA (SÃ³ se Sobrar Tempo/EspaÃ§o)

#### 4. **phi3:medium** â­â­
```bash
ollama pull phi3:medium
```
**Por quÃª:**
- Melhor contexto que phi3:mini
- Mas phi3:mini jÃ¡ estÃ¡ MUITO lento (~267s)
- ~7.9GB (pode causar swap)

**NÃ£o recomendo** atÃ© resolver o problema de latÃªncia.

---

## ðŸ’¡ RecomendaÃ§Ã£o Final

### AÃ§Ã£o Imediata:

**1. Otimizar Ollama:**
```bash
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_THREADS=12
sudo systemctl restart ollama
```

**2. Testar individualmente:**
```bash
python src/experiments/benchmark_models_mlflow.py --test-model ollama::gemma2:2b
```

**Expectativa:** Deveria cair de 63s para ~2-3s

---

**3. Se ainda lento, rodar com apenas 1 modelo:**
```bash
# Remover modelos temporariamente
ollama rm qwen2.5:3b
ollama rm phi3:mini
ollama rm llama3.2:3b

# Testar apenas gemma2:2b
python src/experiments/benchmark_models_mlflow.py --test-model ollama::gemma2:2b

# Se rÃ¡pido agora, problema ERA memÃ³ria!
```

---

### Modelos para Instalar (Depois de Otimizar):

**Se latÃªncia cair para normal (~2-5s):**
```bash
# 1. ESSENCIAL - Melhor raciocÃ­nio
ollama pull mistral:7b-instruct-q4_K_M

# 2. RECOMENDADO - Alta qualidade
ollama pull gemma2:9b

# 3. OPCIONAL - VersÃ£o avanÃ§ada
ollama pull llama3.1:8b-instruct-q4_K_M
```

**ComparaÃ§Ã£o depois do benchmark:**
- mistral vs llama3.2 (qual tem melhor qualidade?)
- gemma2:9b vs gemma2:2b (vale a pena o modelo maior?)

---

## ðŸ“ˆ PrÃ³ximos Passos

1. âœ… **Otimizar Ollama** (configuraÃ§Ãµes de memÃ³ria/threads)
2. âœ… **Testar individualmente** (verificar se latÃªncia normaliza)
3. âœ… **Instalar mistral:7b** (se latÃªncia OK)
4. âœ… **Benchmark novamente** com configuraÃ§Ãµes otimizadas
5. âœ… **Comparar no MLflow** (qualidade vs latÃªncia vs custo)

---

**ðŸ‰ Resultado Esperado ApÃ³s OtimizaÃ§Ã£o:**

| Modelo | LatÃªncia Atual | LatÃªncia Esperada | Status |
|--------|----------------|-------------------|--------|
| gemma2:2b | 63s ðŸ”´ | ~2s âœ… | -97% |
| qwen2.5:3b | 105s ðŸ”´ | ~3s âœ… | -97% |
| llama3.2:3b | 127s ðŸ”´ | ~3s âœ… | -98% |
| **mistral:7b** | N/A | ~6s âœ… | **NOVO** |
| **gemma2:9b** | 0s âŒ | ~8s âœ… | **INSTALAR** |

**DecisÃ£o Final:**
- Se Ollama rÃ¡pido: Usar modelo local (mistral ou llama3.2)
- Se Ollama lento: Usar OpenAI (gpt-4o-mini = 7.89s, ~$0.0004/query)

