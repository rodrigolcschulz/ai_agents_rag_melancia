# üéØ An√°lise de Gap - MelancIA Fine-Tuning & MLOps

**Data**: 19 de Novembro de 2025  
**Status**: üìä An√°lise Completa de Requisitos vs Implementa√ß√£o Atual

---

## üìã SUM√ÅRIO EXECUTIVO

### ‚úÖ O que J√Å est√° implementado (Score: 45%)

| Categoria | Status | Nota |
|-----------|--------|------|
| **RAG Base** | ‚úÖ Completo | 100% - RAG funcional com ChromaDB |
| **MLflow Tracking** | ‚úÖ Completo | 100% - Tracking de experimentos |
| **Multi-LLM Support** | ‚úÖ Completo | 100% - OpenAI, Ollama, HuggingFace |
| **Model Router** | ‚úÖ Completo | 90% - Roteamento inteligente + A/B test |
| **Pipeline ETL** | ‚úÖ Completo | 100% - Scraping + curadoria de dados |
| **Docker** | ‚úÖ Parcial | 70% - Docker Compose b√°sico |
| **Fine-Tuning** | ‚ùå N√£o iniciado | 0% - **CR√çTICO** |
| **Evaluation Loops** | ‚ùå N√£o iniciado | 0% - **CR√çTICO** |
| **CI/CD** | ‚ùå N√£o iniciado | 0% - **IMPORTANTE** |
| **Monitoramento Produ√ß√£o** | ‚ùå N√£o iniciado | 0% - **IMPORTANTE** |
| **API REST (FastAPI)** | ‚ùå N√£o iniciado | 0% - **CR√çTICO** |
| **Testes Automatizados** | ‚ùå N√£o iniciado | 0% - **IMPORTANTE** |

### üéØ Prioridades

**üî¥ CR√çTICO (Sem isso, n√£o atende o escopo)**
1. Fine-Tuning de LLMs (LoRA/QLoRA)
2. API REST com FastAPI
3. Dataset propriet√°rio para fine-tuning
4. Evaluation loops automatizados

**üü° IMPORTANTE (MLOps completo)**
5. CI/CD Pipeline
6. Testes automatizados
7. Monitoramento em produ√ß√£o
8. Model Registry completo
9. Drift Detection

**üü¢ DESEJ√ÅVEL (Polimento)**
10. Integra√ß√£o Vertex AI / Azure ML
11. Data versioning (DVC)
12. Model Cards formais
13. Dashboard de analytics

---

## üî¥ 1. GAPS CR√çTICOS (Bloqueadores)

### 1.1 Fine-Tuning de LLMs ‚ùå

**Status**: N√£o implementado  
**Impacto**: CR√çTICO - √â o core do escopo

**O que falta:**

#### A) Infraestrutura de Fine-Tuning
```python
# src/finetuning/
‚îú‚îÄ‚îÄ trainer.py              # Classe principal de treinamento
‚îú‚îÄ‚îÄ data_preparation.py     # Prepara√ß√£o de datasets
‚îú‚îÄ‚îÄ lora_config.py          # Configura√ß√µes LoRA/QLoRA
‚îú‚îÄ‚îÄ evaluation.py           # Avalia√ß√£o p√≥s-treinamento
‚îî‚îÄ‚îÄ model_loader.py         # Loading de modelos base
```

#### B) Scripts de Treinamento
- ‚ùå Script de fine-tuning com LoRA/QLoRA
- ‚ùå Suporte a PEFT (Parameter-Efficient Fine-Tuning)
- ‚ùå Quantiza√ß√£o (4-bit/8-bit) para economia de mem√≥ria
- ‚ùå Distributed training (se multi-GPU)
- ‚ùå Gradient accumulation
- ‚ùå Mixed precision training (fp16/bf16)

#### C) Dataset para Fine-Tuning
- ‚ùå Dataset propriet√°rio de Retail Media Ads
- ‚ùå Formato de instru√ß√£o (instruction-following)
- ‚ùå Valida√ß√£o e limpeza de dados
- ‚ùå Train/Val/Test split estratificado
- ‚ùå Data augmentation para e-commerce

**Exemplo de dataset necess√°rio:**
```json
{
  "instruction": "Explique o que √© ACOS no contexto de Retail Media",
  "input": "",
  "output": "ACOS (Advertising Cost of Sale) √© a m√©trica que mede...",
  "context": "retail_media_metrics",
  "metadata": {
    "source": "conecta_ads_blog",
    "difficulty": "medium",
    "category": "metricas"
  }
}
```

#### D) T√©cnicas de Fine-Tuning
Implementar:
- ‚úÖ Base j√° tem: PyTorch, Transformers, PEFT
- ‚ùå LoRA (Low-Rank Adaptation)
- ‚ùå QLoRA (Quantized LoRA)
- ‚ùå Prefix Tuning
- ‚ùå Prompt Tuning
- ‚ùå Adapter Layers

#### E) Integra√ß√£o com MLflow
- ‚ùå Log de hiperpar√¢metros de treinamento
- ‚ùå Log de m√©tricas durante treinamento (loss, accuracy)
- ‚ùå Salvamento de checkpoints
- ‚ùå Registro de modelos fine-tunados
- ‚ùå Compara√ß√£o de runs de fine-tuning

**Exemplo de implementa√ß√£o m√≠nima:**
```python
# src/finetuning/trainer.py (PRECISA SER CRIADO)
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import mlflow

class LLMFineTuner:
    """Fine-tuning de LLMs com LoRA/QLoRA"""
    
    def __init__(self, base_model_name: str, output_dir: str):
        self.base_model_name = base_model_name
        self.output_dir = output_dir
        
    def prepare_model(self, use_4bit: bool = True):
        """Carrega modelo base com quantiza√ß√£o"""
        # Implementar loading com bitsandbytes
        pass
    
    def setup_lora(self, r: int = 8, alpha: int = 16):
        """Configura LoRA"""
        lora_config = LoraConfig(
            r=r,  # Rank
            lora_alpha=alpha,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        return lora_config
    
    def train(self, train_dataset, val_dataset, epochs: int = 3):
        """Executa fine-tuning"""
        # Implementar loop de treinamento
        with mlflow.start_run():
            mlflow.log_params({
                "base_model": self.base_model_name,
                "epochs": epochs,
                "lora_r": 8
            })
            
            # Training loop
            for epoch in range(epochs):
                # ...
                mlflow.log_metrics({
                    "train_loss": train_loss,
                    "val_loss": val_loss
                }, step=epoch)
```

---

### 1.2 API REST com FastAPI ‚ùå

**Status**: FastAPI est√° nos requirements, mas n√£o implementado  
**Impacto**: CR√çTICO - Necess√°rio para produ√ß√£o

**O que falta:**

#### A) Estrutura da API
```python
# src/api/
‚îú‚îÄ‚îÄ main.py              # FastAPI app principal
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ query.py        # Endpoints de queries
‚îÇ   ‚îú‚îÄ‚îÄ models.py       # Endpoints de modelos
‚îÇ   ‚îú‚îÄ‚îÄ health.py       # Health checks
‚îÇ   ‚îî‚îÄ‚îÄ admin.py        # Admin/management
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îú‚îÄ‚îÄ request.py      # Pydantic models request
‚îÇ   ‚îî‚îÄ‚îÄ response.py     # Pydantic models response
‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îú‚îÄ‚îÄ auth.py         # Autentica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ rate_limit.py   # Rate limiting
‚îÇ   ‚îî‚îÄ‚îÄ logging.py      # Request logging
‚îî‚îÄ‚îÄ dependencies.py      # FastAPI dependencies
```

#### B) Endpoints Necess√°rios

**Queries (Core)**
```python
POST /api/v1/query
POST /api/v1/query/stream    # Streaming responses
GET  /api/v1/query/{query_id}
```

**Modelos**
```python
GET  /api/v1/models          # Listar modelos dispon√≠veis
POST /api/v1/models/select   # Selecionar modelo
GET  /api/v1/models/status   # Status dos modelos
```

**Health & Monitoring**
```python
GET  /health                 # Health check
GET  /metrics                # Prometheus metrics
GET  /api/v1/stats           # Estat√≠sticas de uso
```

**Admin**
```python
POST /api/v1/finetune/start  # Iniciar fine-tuning
GET  /api/v1/finetune/status # Status do treinamento
POST /api/v1/cache/clear     # Limpar cache
```

#### C) Exemplo de Implementa√ß√£o M√≠nima
```python
# src/api/main.py (PRECISA SER CRIADO)
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import time

from src.mlops.model_router import ModelRouter

app = FastAPI(
    title="MelancIA API",
    version="1.0.0",
    description="API de IA para Retail Media Ads"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global router
router = ModelRouter()

# Schemas
class QueryRequest(BaseModel):
    question: str
    user_tier: str = "free"
    user_id: int = None
    max_tokens: int = 500

class QueryResponse(BaseModel):
    answer: str
    provider: str
    model_name: str
    latency_seconds: float
    success: bool

# Routes
@app.post("/api/v1/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """Processa query do usu√°rio"""
    try:
        result = router.route_query(
            question=request.question,
            user_tier=request.user_tier,
            user_id=request.user_id
        )
        return QueryResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """Health check"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "stats": router.get_stats()
    }

@app.get("/api/v1/models")
async def list_models():
    """Lista modelos dispon√≠veis"""
    return {
        "models": [
            {
                "provider": "openai",
                "name": "gpt-4o-mini",
                "status": "available"
            },
            {
                "provider": "ollama",
                "name": "phi3:mini",
                "status": "available"
            }
        ]
    }
```

#### D) Autentica√ß√£o e Seguran√ßa
- ‚ùå JWT tokens
- ‚ùå API keys
- ‚ùå Rate limiting (por usu√°rio/IP)
- ‚ùå HTTPS/TLS
- ‚ùå Request validation
- ‚ùå CORS configurado

#### E) Documenta√ß√£o API
- ‚ùå OpenAPI/Swagger auto-gerado (FastAPI j√° faz)
- ‚ùå Exemplos de uso
- ‚ùå Postman collection
- ‚ùå Client SDKs (Python, JavaScript)

---

### 1.3 Dataset Propriet√°rio para Fine-Tuning ‚ùå

**Status**: Tem scraping de blog, mas n√£o formatado para fine-tuning  
**Impacto**: CR√çTICO - Sem dados, n√£o tem fine-tuning

**O que falta:**

#### A) Estrutura de Dados
```python
# data/datasets/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ conecta_blog_raw.json       # Scraping bruto
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl                 # 80% treino
‚îÇ   ‚îú‚îÄ‚îÄ val.jsonl                   # 10% valida√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ test.jsonl                  # 10% teste
‚îú‚îÄ‚îÄ synthetic/
‚îÇ   ‚îî‚îÄ‚îÄ augmented_data.jsonl        # Dados sint√©ticos
‚îî‚îÄ‚îÄ metadata/
    ‚îî‚îÄ‚îÄ dataset_info.json            # Metadados do dataset
```

#### B) Formato de Instru√ß√£o
Converter blog posts em pares instru√ß√£o-resposta:

```json
// Exemplo de registro no formato de fine-tuning
{
  "messages": [
    {
      "role": "system",
      "content": "Voc√™ √© a MelancIA, especialista em Retail Media Ads..."
    },
    {
      "role": "user",
      "content": "Como calcular o ROAS de uma campanha no Mercado Livre?"
    },
    {
      "role": "assistant",
      "content": "O ROAS (Return on Ad Spend) √© calculado dividindo..."
    }
  ],
  "metadata": {
    "source": "conecta_blog",
    "category": "metricas",
    "difficulty": "intermediate"
  }
}
```

#### C) Pipeline de Prepara√ß√£o de Dados
```python
# src/finetuning/data_preparation.py (PRECISA SER CRIADO)
from datasets import Dataset, load_dataset
import json
from typing import List, Dict

class RetailMediaDatasetBuilder:
    """Constr√≥i dataset de fine-tuning a partir do blog"""
    
    def __init__(self, raw_data_path: str):
        self.raw_data = self.load_raw_data(raw_data_path)
        
    def convert_blog_to_qa(self) -> List[Dict]:
        """Converte posts do blog em QA pairs"""
        # Implementar:
        # 1. Extrair perguntas dos t√≠tulos/subt√≠tulos
        # 2. Gerar respostas do conte√∫do
        # 3. Validar qualidade
        pass
    
    def generate_synthetic_data(self) -> List[Dict]:
        """Gera dados sint√©ticos com GPT-4"""
        # Usar OpenAI para gerar varia√ß√µes
        pass
    
    def validate_and_clean(self, dataset: List[Dict]) -> List[Dict]:
        """Valida e limpa dataset"""
        # Remover duplicatas, validar formato, etc
        pass
    
    def split_dataset(self, train_ratio=0.8, val_ratio=0.1):
        """Divide em train/val/test"""
        pass
```

#### D) Tipos de Dados Necess√°rios
1. **Conceitos** (O que √© X?)
   - "O que √© Retail Media?"
   - "Explique ACOS"
   
2. **Como Fazer** (Tutoriais)
   - "Como otimizar campanhas no ML?"
   - "Passo a passo para criar an√∫ncios"
   
3. **Compara√ß√µes** (X vs Y)
   - "ACOS vs ROAS: qual usar?"
   - "Mercado Livre vs Shopee"
   
4. **Troubleshooting** (Problemas)
   - "Meu ACOS est√° alto, o que fazer?"
   - "Como reduzir CPC?"
   
5. **Estrat√©gias** (Conselhos)
   - "Melhores pr√°ticas para Black Friday"
   - "Como aumentar convers√£o"

#### E) Valida√ß√£o de Qualidade
```python
# src/finetuning/data_validation.py (PRECISA SER CRIADO)
class DatasetValidator:
    """Valida qualidade do dataset"""
    
    def check_diversity(self, dataset):
        """Verifica diversidade de exemplos"""
        pass
    
    def check_length_distribution(self, dataset):
        """Analisa distribui√ß√£o de tamanhos"""
        pass
    
    def check_balance(self, dataset):
        """Verifica balanceamento de categorias"""
        pass
    
    def detect_duplicates(self, dataset):
        """Detecta duplicatas/near-duplicates"""
        pass
```

---

### 1.4 Evaluation Loops Automatizados ‚ùå

**Status**: Tem benchmark manual, mas n√£o loops automatizados  
**Impacto**: CR√çTICO - Necess√°rio para validar fine-tuning

**O que falta:**

#### A) Framework de Avalia√ß√£o
```python
# src/evaluation/
‚îú‚îÄ‚îÄ evaluator.py        # Classe principal
‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îú‚îÄ‚îÄ accuracy.py     # M√©tricas de accuracy
‚îÇ   ‚îú‚îÄ‚îÄ semantic.py     # Similaridade sem√¢ntica
‚îÇ   ‚îú‚îÄ‚îÄ rouge.py        # ROUGE scores
‚îÇ   ‚îî‚îÄ‚îÄ bertscore.py    # BERTScore
‚îú‚îÄ‚îÄ benchmarks/
‚îÇ   ‚îú‚îÄ‚îÄ retail_media_benchmark.json  # Benchmark propriet√°rio
‚îÇ   ‚îî‚îÄ‚îÄ generic_benchmark.json       # Benchmark gen√©rico
‚îî‚îÄ‚îÄ reports/
    ‚îî‚îÄ‚îÄ report_generator.py
```

#### B) M√©tricas de Avalia√ß√£o
Implementar:
- ‚úÖ J√° tem: Qualidade e relev√¢ncia b√°sicas
- ‚ùå ROUGE-L (overlap de n-gramas)
- ‚ùå BERTScore (similaridade sem√¢ntica)
- ‚ùå Exact Match
- ‚ùå F1 Score
- ‚ùå Perplexity
- ‚ùå Human evaluation score

#### C) Benchmark Propriet√°rio
Criar dataset de teste com respostas corretas:

```json
// data/evaluation/retail_media_benchmark.json
{
  "name": "Retail Media Benchmark",
  "version": "1.0",
  "questions": [
    {
      "id": "rm001",
      "question": "O que √© ACOS?",
      "reference_answer": "ACOS (Advertising Cost of Sale) √© a m√©trica...",
      "category": "metrics",
      "difficulty": "easy",
      "keywords": ["ACOS", "custo", "venda", "m√©trica"]
    }
    // ... mais 100-500 quest√µes
  ]
}
```

#### D) Loop de Avalia√ß√£o
```python
# src/evaluation/evaluator.py (PRECISA SER CRIADO)
from typing import Dict, List
import mlflow
from rouge_score import rouge_scorer
from bert_score import BERTScorer

class ModelEvaluator:
    """Avalia modelos fine-tunados contra benchmark"""
    
    def __init__(self, benchmark_path: str):
        self.benchmark = self.load_benchmark(benchmark_path)
        self.rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])
        self.bert = BERTScorer(lang='pt')
        
    def evaluate_model(self, model, model_name: str) -> Dict:
        """
        Avalia modelo em todo o benchmark
        
        Returns:
            Dict com todas as m√©tricas
        """
        results = []
        
        for item in self.benchmark['questions']:
            # Gerar resposta
            answer = model.invoke(item['question'])
            
            # Calcular m√©tricas
            rouge_scores = self.rouge.score(
                item['reference_answer'],
                answer
            )
            
            bert_score = self.bert.score(
                [answer],
                [item['reference_answer']]
            )
            
            results.append({
                'question_id': item['id'],
                'rouge_l': rouge_scores['rougeL'].fmeasure,
                'bert_score': bert_score[2].item(),  # F1
                'category': item['category']
            })
        
        # Agregar m√©tricas
        metrics = self._aggregate_metrics(results)
        
        # Log no MLflow
        with mlflow.start_run(run_name=f"eval_{model_name}"):
            mlflow.log_metrics(metrics)
            mlflow.log_artifact("eval_results.json")
        
        return metrics
    
    def _aggregate_metrics(self, results: List[Dict]) -> Dict:
        """Agrega m√©tricas por categoria e geral"""
        # Calcular m√©dias, std, por categoria
        pass
```

#### E) Avalia√ß√£o Cont√≠nua
- ‚ùå Avalia√ß√£o ap√≥s cada fine-tuning
- ‚ùå Compara√ß√£o com baseline
- ‚ùå Detec√ß√£o de regress√£o
- ‚ùå Alertas se performance cair

```python
# Pipeline de avalia√ß√£o automatizado
def automated_evaluation_loop():
    """
    Loop que roda ap√≥s cada fine-tuning
    """
    # 1. Carregar modelo fine-tunado
    model = load_finetuned_model()
    
    # 2. Avaliar
    evaluator = ModelEvaluator("benchmark.json")
    metrics = evaluator.evaluate_model(model)
    
    # 3. Comparar com baseline
    baseline_metrics = load_baseline_metrics()
    
    if metrics['rouge_l'] < baseline_metrics['rouge_l']:
        send_alert("‚ö†Ô∏è Regress√£o detectada!")
    
    # 4. Se melhor, promover
    if metrics['bert_score'] > baseline_metrics['bert_score']:
        promote_to_production(model)
```

---

## üü° 2. GAPS IMPORTANTES (MLOps Completo)

### 2.1 CI/CD Pipeline ‚ùå

**O que falta:**
- ‚ùå GitHub Actions / GitLab CI
- ‚ùå Testes automatizados em cada commit
- ‚ùå Build e push de Docker images
- ‚ùå Deploy autom√°tico para staging
- ‚ùå Smoke tests p√≥s-deploy
- ‚ùå Rollback autom√°tico

**Exemplo m√≠nimo:**
```yaml
# .github/workflows/ci-cd.yml (PRECISA SER CRIADO)
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: |
          pip install -r requirements.txt
          pytest tests/ -v
      
      - name: Run linter
        run: |
          black --check src/
          flake8 src/
  
  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Build Docker image
        run: docker build -t melancia:${{ github.sha }} .
      
      - name: Push to registry
        run: docker push melancia:${{ github.sha }}
  
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to production
        run: |
          kubectl set image deployment/melancia melancia=melancia:${{ github.sha }}
```

---

### 2.2 Testes Automatizados ‚ùå

**O que falta:**

```python
# tests/ (PRECISA SER EXPANDIDO)
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_model_router.py
‚îÇ   ‚îú‚îÄ‚îÄ test_benchmark.py
‚îÇ   ‚îú‚îÄ‚îÄ test_finetuning.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_end_to_end.py
‚îÇ   ‚îú‚îÄ‚îÄ test_mlflow_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api_integration.py
‚îú‚îÄ‚îÄ load/
‚îÇ   ‚îî‚îÄ‚îÄ test_load_performance.py
‚îî‚îÄ‚îÄ conftest.py  # Fixtures compartilhados
```

**Exemplos de testes necess√°rios:**

```python
# tests/unit/test_model_router.py (PRECISA SER CRIADO)
import pytest
from src.mlops.model_router import ModelRouter, FeatureFlags

def test_router_escolhe_openai_para_premium():
    """Premium users devem sempre usar OpenAI"""
    router = ModelRouter(enable_tracking=False)
    
    decision = router.decide_routing(
        question="Teste",
        user_tier="premium"
    )
    
    assert decision.provider == "openai"
    assert decision.reason == "premium_user"

def test_router_respeita_a_b_test():
    """A/B test deve ser consistente por user_id"""
    router = ModelRouter(enable_tracking=False)
    
    # Mesmo user_id deve ter mesma experi√™ncia
    decision1 = router.decide_routing(
        question="Teste",
        user_tier="free",
        user_id=123
    )
    
    decision2 = router.decide_routing(
        question="Outra pergunta",
        user_tier="free",
        user_id=123
    )
    
    assert decision1.provider == decision2.provider

# tests/integration/test_api_integration.py (PRECISA SER CRIADO)
from fastapi.testclient import TestClient
from src.api.main import app

client = TestClient(app)

def test_query_endpoint():
    """Testa endpoint de query"""
    response = client.post(
        "/api/v1/query",
        json={
            "question": "O que √© Retail Media?",
            "user_tier": "free"
        }
    )
    
    assert response.status_code == 200
    data = response.json()
    assert "answer" in data
    assert data["success"] == True

def test_health_endpoint():
    """Health check deve retornar 200"""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"
```

**Coverage target: > 80%**

---

### 2.3 Monitoramento em Produ√ß√£o ‚ùå

**O que falta:**

#### A) Prometheus + Grafana
```yaml
# docker-compose.monitoring.yml (PRECISA SER CRIADO)
version: '3.8'

services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
  
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

volumes:
  prometheus_data:
  grafana_data:
```

#### B) M√©tricas Customizadas
```python
# src/monitoring/metrics.py (PRECISA SER CRIADO)
from prometheus_client import Counter, Histogram, Gauge

# M√©tricas de queries
query_counter = Counter(
    'melancia_queries_total',
    'Total de queries processadas',
    ['provider', 'user_tier']
)

query_latency = Histogram(
    'melancia_query_latency_seconds',
    'Lat√™ncia de queries',
    ['provider']
)

active_users = Gauge(
    'melancia_active_users',
    'Usu√°rios ativos'
)

model_errors = Counter(
    'melancia_model_errors_total',
    'Erros de modelo',
    ['provider', 'error_type']
)

# Uso na API
@app.post("/api/v1/query")
async def query(request: QueryRequest):
    query_counter.labels(
        provider='openai',
        user_tier=request.user_tier
    ).inc()
    
    with query_latency.labels(provider='openai').time():
        result = router.route_query(request.question)
    
    return result
```

#### C) Dashboards
- ‚ùå Dashboard de lat√™ncia (p50, p95, p99)
- ‚ùå Dashboard de custos
- ‚ùå Dashboard de uso (queries/hora)
- ‚ùå Dashboard de erros
- ‚ùå Dashboard de A/B test

---

### 2.4 Model Registry Completo ‚ùå

**O que falta:**

```python
# src/mlops/registry.py - EXPANDIR
class ModelRegistry:
    """J√° existe, mas precisa de features"""
    
    # Adicionar:
    def promote_to_staging(self, model_name, version):
        """Promove modelo para staging"""
        pass
    
    def promote_to_production(self, model_name, version):
        """Promove modelo para produ√ß√£o"""
        # Valida√ß√µes:
        # 1. Passou nos testes?
        # 2. Performance aceit√°vel?
        # 3. Aprova√ß√£o humana?
        pass
    
    def rollback(self, model_name, to_version):
        """Rollback para vers√£o anterior"""
        pass
    
    def compare_models(self, model1, model2):
        """Compara dois modelos"""
        pass
```

**Workflow necess√°rio:**
```
Fine-tuning ‚Üí Evaluation ‚Üí Staging ‚Üí A/B Test ‚Üí Production
                  ‚Üì            ‚Üì          ‚Üì         ‚Üì
                Fails?     Fails?     Worse?    Issues?
                  ‚Üì            ‚Üì          ‚Üì         ‚Üì
                Stop       Stop      Rollback   Rollback
```

---

### 2.5 Drift Detection ‚ùå

**O que falta:**

```python
# src/monitoring/drift_detection.py (PRECISA SER CRIADO)
from scipy.stats import ks_2samp
import numpy as np

class DriftDetector:
    """
    Detecta mudan√ßas na distribui√ß√£o de dados/m√©tricas
    """
    
    def __init__(self):
        self.baseline_latencies = []
        self.baseline_quality_scores = []
        
    def collect_baseline(self, window_hours: int = 24):
        """Coleta m√©tricas baseline"""
        # Coletar √∫ltimas 24h como baseline
        pass
    
    def detect_latency_drift(self, current_latencies):
        """Detecta drift em lat√™ncia"""
        ks_stat, p_value = ks_2samp(
            self.baseline_latencies,
            current_latencies
        )
        
        if p_value < 0.05:
            return {
                "drift_detected": True,
                "metric": "latency",
                "ks_statistic": ks_stat,
                "p_value": p_value
            }
        
        return {"drift_detected": False}
    
    def detect_quality_drift(self, current_scores):
        """Detecta drift em qualidade"""
        # Comparar distribui√ß√£o de scores
        pass
    
    def detect_input_drift(self, current_questions):
        """Detecta mudan√ßa no tipo de perguntas"""
        # Analisar embeddings das perguntas
        pass
```

**Alertas:**
- ‚ùå Email/Slack quando drift detectado
- ‚ùå Auto-retreinamento se drift persistir
- ‚ùå Dashboard de drift

---

## üü¢ 3. GAPS DESEJ√ÅVEIS (Polimento)

### 3.1 Integra√ß√£o Cloud MLOps ‚ùå

**Vertex AI (Google Cloud):**
```python
# src/cloud/vertex_ai.py (PRECISA SER CRIADO)
from google.cloud import aiplatform

def deploy_to_vertex_ai(model_path: str):
    """Deploy modelo para Vertex AI"""
    aiplatform.init(project='conecta-ads')
    
    model = aiplatform.Model.upload(
        display_name="melancia-llm",
        artifact_uri=model_path
    )
    
    endpoint = model.deploy(
        machine_type="n1-standard-4",
        accelerator_type="NVIDIA_TESLA_T4",
        accelerator_count=1
    )
```

**Azure ML:**
```python
# src/cloud/azure_ml.py (PRECISA SER CRIADO)
from azureml.core import Workspace, Model

def deploy_to_azure_ml(model_path: str):
    """Deploy modelo para Azure ML"""
    ws = Workspace.from_config()
    
    model = Model.register(
        workspace=ws,
        model_name="melancia-llm",
        model_path=model_path
    )
    
    # Deploy to Azure ML endpoint
```

---

### 3.2 Data Versioning (DVC) ‚ùå

```bash
# .dvc/ (PRECISA SER CONFIGURADO)
# data/ ser√° versionado com DVC

# Comandos necess√°rios:
dvc init
dvc add data/datasets/
dvc remote add -d storage s3://melancia-data
dvc push
```

**Benef√≠cios:**
- Versionamento de datasets grandes
- Reprodutibilidade de experimentos
- Rastreabilidade de mudan√ßas em dados

---

### 3.3 Model Cards ‚ùå

```markdown
# model_cards/llama-3-2-3b-retail-media-v1.md (PRECISA SER CRIADO)

# Model Card: Llama 3.2 3B - Retail Media v1

## Informa√ß√µes do Modelo
- **Nome**: llama-3-2-3b-retail-media-v1
- **Base Model**: meta-llama/Llama-3.2-3B-Instruct
- **Fine-tuning**: LoRA (r=16, alpha=32)
- **Treinado em**: 19/11/2025
- **Dataset**: Conecta Ads Blog (5,000 exemplos)

## Performance
| M√©trica | Valor | Baseline |
|---------|-------|----------|
| ROUGE-L | 0.68 | 0.45 |
| BERTScore | 0.82 | 0.71 |
| Lat√™ncia (p95) | 3.2s | 150s |

## Casos de Uso
‚úÖ **Recomendado:**
- Perguntas sobre m√©tricas de Retail Media
- Estrat√©gias de otimiza√ß√£o de campanhas
- Compara√ß√µes entre plataformas

‚ùå **N√£o Recomendado:**
- An√°lise financeira complexa
- Recomenda√ß√µes legais
- Decis√µes cr√≠ticas sem revis√£o humana

## Limita√ß√µes Conhecidas
- Performance inferior em perguntas muito t√©cnicas
- Pode alucinar n√∫meros/estat√≠sticas
- Vi√©s para Mercado Livre (mais exemplos no dataset)

## Considera√ß√µes √âticas
- Sempre indicar que √© resposta gerada por IA
- N√£o usar para decis√µes cr√≠ticas sem revis√£o
- Monitora para vi√©s e fairness

## Contato
- Respons√°vel: Equipe MelancIA
- Email: ai@conectaads.com.br
```

---

### 3.4 Dashboard de Analytics ‚ùå

```python
# src/dashboard/app.py (PRECISA SER CRIADO)
import streamlit as st
import plotly.express as px

st.title("üçâ MelancIA Analytics Dashboard")

# M√©tricas principais
col1, col2, col3 = st.columns(3)
col1.metric("Queries Hoje", "1,234", "+12%")
col2.metric("Lat√™ncia M√©dia", "4.2s", "-0.5s")
col3.metric("Custo Hoje", "$15.23", "+$2.10")

# Gr√°ficos
st.plotly_chart(latency_over_time_chart)
st.plotly_chart(provider_distribution_chart)
st.plotly_chart(user_tier_breakdown_chart)
```

---

## üìä 4. ROADMAP DE IMPLEMENTA√á√ÉO

### Sprint 1 (Semanas 1-2): Fundamentos de Fine-Tuning
**Objetivo**: Primeiro modelo fine-tunado funcionando

- [ ] Criar `src/finetuning/` com estrutura b√°sica
- [ ] Implementar `trainer.py` com LoRA
- [ ] Preparar dataset inicial (100 exemplos manuais)
- [ ] Fine-tunar Phi-3-mini ou Llama-3.2-3b
- [ ] Integrar com MLflow tracking
- [ ] Avaliar modelo fine-tunado vs baseline

**Entreg√°vel**: Modelo fine-tunado com m√©tricas documentadas

---

### Sprint 2 (Semanas 3-4): API REST
**Objetivo**: API em produ√ß√£o

- [ ] Criar `src/api/` com FastAPI
- [ ] Implementar endpoints principais
- [ ] Adicionar autentica√ß√£o b√°sica
- [ ] Integrar com ModelRouter
- [ ] Dockerizar API
- [ ] Deploy em staging

**Entreg√°vel**: API funcional e documentada

---

### Sprint 3 (Semanas 5-6): Dataset e Evaluation
**Objetivo**: Dataset robusto e avalia√ß√£o automatizada

- [ ] Converter blog completo em dataset
- [ ] Gerar 500+ exemplos de fine-tuning
- [ ] Criar benchmark de avalia√ß√£o (100 Q&A)
- [ ] Implementar `src/evaluation/evaluator.py`
- [ ] Integrar ROUGE, BERTScore
- [ ] Loop de avalia√ß√£o ap√≥s fine-tuning

**Entreg√°vel**: Dataset de 1000+ exemplos + sistema de avalia√ß√£o

---

### Sprint 4 (Semanas 7-8): Testes e CI/CD
**Objetivo**: Pipeline automatizado

- [ ] Criar suite de testes (>80% coverage)
- [ ] Setup GitHub Actions
- [ ] Testes automatizados em cada commit
- [ ] Build/push Docker autom√°tico
- [ ] Deploy autom√°tico para staging
- [ ] Smoke tests p√≥s-deploy

**Entreg√°vel**: CI/CD funcional

---

### Sprint 5 (Semanas 9-10): Monitoramento
**Objetivo**: Observabilidade completa

- [ ] Setup Prometheus + Grafana
- [ ] M√©tricas customizadas
- [ ] Dashboards de produ√ß√£o
- [ ] Alertas configurados
- [ ] Drift detection b√°sico

**Entreg√°vel**: Sistema de monitoramento em produ√ß√£o

---

### Sprint 6 (Semanas 11-12): Produ√ß√£o
**Objetivo**: Sistema completo em produ√ß√£o

- [ ] Re-treinamento peri√≥dico automatizado
- [ ] A/B testing em produ√ß√£o
- [ ] Model registry completo
- [ ] Documenta√ß√£o completa (Model Cards)
- [ ] Otimiza√ß√µes de performance
- [ ] Security audit

**Entreg√°vel**: Sistema MLOps completo

---

## üéØ 5. PR√ìXIMOS PASSOS IMEDIATOS

### Esta Semana (Prioridade M√ÅXIMA):

#### 1. Prototipar Fine-Tuning (2-3 dias)
```bash
# Criar script b√°sico de fine-tuning
cd src/finetuning
touch trainer.py data_preparation.py

# Preparar 50-100 exemplos manuais
# Fine-tunar Phi-3-mini com LoRA
# Avaliar se funciona
```

#### 2. Criar API B√°sica (2-3 dias)
```bash
# Criar API m√≠nima funcional
cd src/api
touch main.py

# Implementar 2-3 endpoints essenciais
# Testar localmente
```

#### 3. Preparar Dataset Inicial (1-2 dias)
```bash
# Converter 100 posts do blog
# Formatar para fine-tuning
# Validar qualidade
```

---

## üìã 6. CHECKLIST DE VALIDA√á√ÉO

Antes de considerar o projeto "completo":

### Fine-Tuning
- [ ] Modelo fine-tunado melhora > 20% vs baseline
- [ ] Pipeline de treinamento reproduz√≠vel
- [ ] Hiperpar√¢metros otimizados
- [ ] Checkpoints salvos no MLflow
- [ ] Tempo de treinamento < 2 horas

### API
- [ ] Lat√™ncia p95 < 5 segundos
- [ ] Rate limit implementado
- [ ] Autentica√ß√£o funcionando
- [ ] Documenta√ß√£o OpenAPI completa
- [ ] Testes de integra√ß√£o passando

### MLOps
- [ ] CI/CD funcionando
- [ ] Coverage de testes > 80%
- [ ] Monitoramento em produ√ß√£o
- [ ] Alertas configurados
- [ ] Drift detection ativo

### Dataset
- [ ] > 1000 exemplos de fine-tuning
- [ ] Train/Val/Test splits corretos
- [ ] Benchmark de avalia√ß√£o (100+ Q&A)
- [ ] Diversidade de t√≥picos
- [ ] Qualidade validada

### Documenta√ß√£o
- [ ] README atualizado
- [ ] Model Cards criados
- [ ] API documentation
- [ ] Runbook de opera√ß√µes
- [ ] Guia de troubleshooting

---

## üí∞ 7. ESTIMATIVA DE ESFOR√áO

| Categoria | Sprints | Dias | Pessoas |
|-----------|---------|------|---------|
| Fine-Tuning | 1 | 10 | 1-2 |
| API REST | 1 | 10 | 1 |
| Dataset | 1 | 10 | 1-2 |
| CI/CD + Testes | 1 | 10 | 1 |
| Monitoramento | 1 | 10 | 1 |
| Produ√ß√£o | 1 | 10 | 2 |
| **TOTAL** | **6** | **60** | **1-2** |

**Timeline**: 3 meses (com 1-2 devs dedicados)

---

## üöÄ 8. RECOMENDA√á√ïES FINAIS

### Come√ßar por:
1. ‚úÖ **Fine-Tuning MVP** - √â o core do projeto
2. ‚úÖ **API REST** - Necess√°rio para produ√ß√£o
3. ‚úÖ **Dataset Inicial** - Sem dados, n√£o tem IA

### Pode esperar:
- Cloud integrations (Vertex/Azure)
- DVC para versionamento
- Dashboard fancy de analytics

### Nunca pular:
- Testes automatizados
- Monitoramento b√°sico
- Documenta√ß√£o

---

**Pr√≥xima Revis√£o**: Ap√≥s implementa√ß√£o do Sprint 1  
**Contato**: Equipe MelancIA

