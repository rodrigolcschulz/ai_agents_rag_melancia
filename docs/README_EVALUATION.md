# ğŸ”„ Como Usar Evaluation Loops

## ğŸš€ Quick Start

### 1. Subir o Docker com Evaluation

```bash
# Descer containers antigos
docker-compose down

# Subir com a nova interface (com evaluation)
docker-compose up -d

# Ver logs
docker-compose logs -f melancia-ai
```

### 2. Acessar Interface

Abra no navegador: **http://localhost:8000**

## ğŸ¯ Funcionalidades Novas

### âœ… Logging AutomÃ¡tico
Todas interaÃ§Ãµes sÃ£o automaticamente logadas:
- Query do usuÃ¡rio
- Resposta gerada
- Documentos recuperados
- Modelo usado
- LatÃªncia
- Timestamp

### âœ… MÃ©tricas AutomÃ¡ticas (RAGAS)
Cada resposta Ã© avaliada automaticamente:
- **Faithfulness**: Resposta baseada nos documentos?
- **Answer Relevancy**: Responde realmente a pergunta?
- **Context Relevancy**: Documentos relevantes?

### âœ… Feedback de UsuÃ¡rios
Na interface vocÃª verÃ¡:
- BotÃ£o **ğŸ‘ Sim, ajudou!** - Feedback positivo
- BotÃ£o **ğŸ‘ NÃ£o ajudou** - Feedback negativo  
- Campo para comentÃ¡rios opcionais

### âœ… EstatÃ­sticas em Tempo Real
Painel lateral mostra:
- Total de interaÃ§Ãµes
- LatÃªncia mÃ©dia
- Rating mÃ©dio
- Contagem de feedback positivo/negativo

## ğŸ“ Dados Armazenados

Todos os dados ficam em:
```
data/evaluation/interactions.db
```

Ã‰ um banco SQLite com 4 tabelas:
- `interactions` - Queries e respostas
- `retrieved_docs` - Documentos recuperados
- `feedback` - Feedback dos usuÃ¡rios
- `metrics` - MÃ©tricas calculadas

## ğŸ” Analisar Dados

### Via Python

```python
from src.evaluation.interaction_logger import InteractionLogger

logger = InteractionLogger()

# Ver estatÃ­sticas
stats = logger.get_stats()
print(f"Total: {stats['total_interactions']}")
print(f"Rating mÃ©dio: {stats['avg_rating']}")

# Ver interaÃ§Ãµes recentes
recent = logger.get_recent_interactions(limit=10)
for interaction in recent:
    print(f"{interaction['query']} -> {interaction['response'][:50]}...")
```

### Via SQL

```bash
# Abrir banco
sqlite3 data/evaluation/interactions.db

# Ver feedback negativo (para revisar)
SELECT i.query, i.response, f.rating, f.comment
FROM interactions i
JOIN feedback f ON i.interaction_id = f.interaction_id
WHERE f.feedback_type = 'negative'
ORDER BY f.timestamp DESC;

# MÃ©tricas mÃ©dias
SELECT 
    AVG(metric_value) as avg
FROM metrics
WHERE metric_name = 'faithfulness';

# Queries mais lentas
SELECT query, latency_seconds
FROM interactions
ORDER BY latency_seconds DESC
LIMIT 10;
```

## ğŸ¯ Loop de Melhoria ContÃ­nua

### Processo Semanal

1. **Revisar Feedback Negativo**
```python
# Script para anÃ¡lise
import sqlite3

conn = sqlite3.connect("data/evaluation/interactions.db")
cursor = conn.cursor()

cursor.execute("""
    SELECT i.query, i.response, f.comment
    FROM interactions i
    JOIN feedback f ON i.interaction_id = f.interaction_id
    WHERE f.feedback_type = 'negative'
    AND f.timestamp > datetime('now', '-7 days')
""")

for row in cursor.fetchall():
    print(f"\nQuery: {row[0]}")
    print(f"Resposta: {row[1][:100]}...")
    print(f"ComentÃ¡rio: {row[2]}")
```

2. **Identificar PadrÃµes**
- Perguntas que sempre tÃªm feedback negativo?
- TÃ³picos faltando na documentaÃ§Ã£o?
- Queries confusas?

3. **Melhorar**
- Adicionar/melhorar markdowns
- Ajustar prompts
- Refinar parÃ¢metros do RAG

4. **Testar**
- Fazer as mesmas queries problemÃ¡ticas
- Verificar se melhorou

## ğŸ”§ Customizar

### Desabilitar Evaluation

Edite `docker-compose.yml`:
```yaml
command: python -m src.agent.web_interface  # Interface antiga (sem evaluation)
```

### Mostrar MÃ©tricas nas Respostas

Edite `src/agent/web_interface_with_eval.py`:
```python
melancia = MelanciaWithEvaluation(
    enable_eval=True,
    enable_metrics_display=True  # Mudar para True
)
```

### Mudar Local do Banco

Edite `src/agent/web_interface_with_eval.py`:
```python
self.logger = InteractionLogger(
    db_path="caminho/customizado/interactions.db"
)
```

## ğŸ“Š MÃ©tricas Explicadas

### Faithfulness (0.0 - 1.0)
Mede se a resposta estÃ¡ "fiel" aos documentos recuperados.
- **ğŸŸ¢ > 0.7**: Resposta bem baseada nos docs
- **ğŸŸ¡ 0.4-0.7**: Parcialmente baseada
- **ğŸ”´ < 0.4**: Muita alucinaÃ§Ã£o

### Answer Relevancy (0.0 - 1.0)
Mede se a resposta realmente responde a pergunta.
- **ğŸŸ¢ > 0.7**: Responde bem
- **ğŸŸ¡ 0.4-0.7**: Responde parcialmente
- **ğŸ”´ < 0.4**: NÃ£o responde

### Context Relevancy (0.0 - 1.0)
Mede se os documentos recuperados sÃ£o relevantes.
- **ğŸŸ¢ > 0.7**: Docs muito relevantes
- **ğŸŸ¡ 0.4-0.7**: Parcialmente relevantes
- **ğŸ”´ < 0.4**: Docs nÃ£o relevantes

**Nota**: Estas sÃ£o mÃ©tricas heurÃ­sticas simples. Para maior precisÃ£o, implementar LLM-based evaluation (futuro).

## ğŸš§ PrÃ³ximos Passos

- [ ] Dashboard Streamlit para visualizaÃ§Ãµes
- [ ] Alertas automÃ¡ticos (email) se qualidade cair
- [ ] A/B testing de prompts/parÃ¢metros
- [ ] LLM-based evaluation (mais preciso)
- [ ] RelatÃ³rios semanais automÃ¡ticos
- [ ] IntegraÃ§Ã£o com MLflow

## ğŸ†˜ Troubleshooting

### Banco nÃ£o criado
```bash
# Criar pasta manualmente
mkdir -p data/evaluation
```

### Import errors
```bash
# Rebuild container
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

### Ver logs
```bash
docker-compose logs -f melancia-ai
```

---

**ğŸ‰ MelancIA com Evaluation Loops** - Melhoria contÃ­nua baseada em dados!

