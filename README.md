# ğŸ‰ MelancIA - AI RAG Agente de Product Ads

**MelancIA** Ã© um agente de IA especializado em Product Ads e E-commerce, desenvolvido pela Conecta Ads.

## ğŸš€ Funcionalidades

- **RAG (Retrieval-Augmented Generation)** com base de conhecimento sobre Retail Media
- **Scraping automÃ¡tico** de conteÃºdo do blog Conecta Ads
- **Interface web** interativa com Gradio
- **Pipeline ETL** completo para processamento de dados
- **AnÃ¡lise de conteÃºdo** e geraÃ§Ã£o de relatÃ³rios

## ğŸ› ï¸ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8+
- OpenAI API Key

### Setup Local

```bash
# Clone o repositÃ³rio
git clone https://github.com/conectaads/melancia-ai-rag.git
cd melancia-ai-rag

# Crie e ative o ambiente virtual
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Instale as dependÃªncias
pip install -r requirements.txt

# Configure a API Key
echo "OPENAI_API_KEY=sua_api_key_aqui" > .env
```

## ğŸ¯ Como Usar

### Interface Web

```bash
python src/agent/web_interface.py
```

Acesse: http://localhost:8000

### Terminal

```bash
python src/agent/main.py
```

### Pipeline ETL

```bash
# Scraping + AnÃ¡lise
python src/etl/run_etl.py

# Apenas scraping
python src/etl/run_etl.py --no-analyze

# Limitar artigos
python src/etl/run_etl.py --max-articles 10
```

## ğŸ³ Docker

```bash
docker compose up -d
```

## ğŸ“ Estrutura do Projeto

```text
melancia-ai-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          # Agente RAG principal
â”‚   â”œâ”€â”€ etl/            # Pipeline ETL
â”‚   â”œâ”€â”€ experiments/    # ğŸ†• ExperimentaÃ§Ã£o com LLMs
â”‚   â”‚   â”œâ”€â”€ multi_llm.py      # Gerenciador de mÃºltiplos LLMs
â”‚   â”‚   â”œâ”€â”€ benchmark.py      # Sistema de benchmark
â”‚   â”‚   â””â”€â”€ run_experiments.py # Script principal
â”‚   â””â”€â”€ mlops/          # ğŸ†• MLOps e tracking
â”‚       â”œâ”€â”€ tracking.py       # MLflow tracking
â”‚       â””â”€â”€ registry.py       # Model registry
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/          # Arquivos markdown
â”‚   â”œâ”€â”€ output/         # RelatÃ³rios e anÃ¡lises
â”‚   â”œâ”€â”€ vector_db/      # Base vetorial (ChromaDB)
â”‚   â”œâ”€â”€ experiments/    # ğŸ†• Resultados de benchmarks
â”‚   â””â”€â”€ models/         # ğŸ†• Modelos treinados
â”œâ”€â”€ notebooks/          # ğŸ†• Jupyter notebooks
â”‚   â””â”€â”€ experimentacao_llms.ipynb
â”œâ”€â”€ mlruns/             # ğŸ†• MLflow tracking data
â”œâ”€â”€ logs/               # Logs do sistema
â””â”€â”€ requirements.txt    # DependÃªncias
```

## ğŸ“ Sistema de Logs

O MelancIA mantÃ©m um sistema completo de logs:

- **`logs/chat_history.txt`** - HistÃ³rico completo de conversas
- **`data/output/chat_history.pkl`** - MemÃ³ria da conversa (Ãºltimas 5 interaÃ§Ãµes)
- **`logs/etl_pipeline.log`** - Logs do pipeline ETL
- **`logs/scraper.log`** - Logs do web scraping
- **`logs/vector_db.log`** - Logs da base vetorial

> **Nota**: Todos os arquivos de log sÃ£o ignorados pelo Git (`.gitignore`)

## ğŸ§  Como Funciona o RAG

O MelancIA utiliza uma arquitetura RAG (Retrieval-Augmented Generation) sofisticada:

1. **ğŸ“š Base de Conhecimento**: ConteÃºdo do blog Conecta Ads em formato Markdown
2. **ğŸ” Embeddings**: Transforma o conteÃºdo em vetores usando OpenAI embeddings
3. **ğŸ’¾ Banco Vetorial**: Armazena os vetores no ChromaDB para busca rÃ¡pida
4. **ğŸ¤– LLM**: GPT-4o-mini gera respostas baseadas no contexto recuperado
5. **ğŸ”„ MemÃ³ria**: MantÃ©m contexto das Ãºltimas 5 conversas
6. **ğŸ¯ Filtros**: SÃ³ responde perguntas relevantes sobre Retail Media

## ğŸ¤– Sobre o MelancIA

MelancIA Ã© especializado em:
- **Retail Media** e estratÃ©gias de anÃºncios
- **E-commerce** e marketplaces (Mercado Livre, Shopee)
- **MÃ©tricas de performance** (ACOS, ROAS, CTR, CPC)
- **LogÃ­stica** e fulfillment
- **AnÃ¡lise de concorrÃªncia** e tendÃªncias

## ğŸ§ª ExperimentaÃ§Ã£o com LLMs Open Source

O MelÃ¢ncIA agora suporta mÃºltiplos provedores de LLM para experimentaÃ§Ã£o e comparaÃ§Ã£o!

### ğŸ’» Hardware Testado

Este projeto foi otimizado para rodar em:
- **CPU**: AMD Ryzen 5 3600X (6 cores, 12 threads)
- **RAM**: 15 GB
- **Disco**: 439 GB (346 GB disponÃ­veis)
- **GPU**: AMD Radeon RX 570/580 (CPU inference recomendado)

âœ… **ViÃ¡vel**: Modelos 2-7B quantizados (Phi-3, Llama 3.2, Gemma)
âš ï¸ **NÃ£o recomendado**: Modelos 13B+, GPU training

ğŸ“– **Ver**: [docs/HARDWARE_SETUP.md](docs/HARDWARE_SETUP.md) para anÃ¡lise completa

### Provedores Suportados

**ğŸ¤– OpenAI** (Pago - Alta Qualidade)
- gpt-4o-mini
- gpt-4o
- gpt-3.5-turbo

**ğŸ¦™ Ollama** (Gratuito - Local)
- llama3.1:8b / llama3.1:70b
- mistral:7b
- phi3:mini
- gemma2:9b
- qwen2.5:7b

**ğŸ¤— HuggingFace** (Gratuito - API)
- mistralai/Mistral-7B-Instruct-v0.2
- meta-llama/Llama-2-7b-chat-hf
- tiiuae/falcon-7b-instruct

### ğŸš€ Quick Start - Experimentos

#### 1. Instalar Ollama (Recomendado para testes locais)

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo
ollama pull llama3.1:8b

# Verificar se estÃ¡ rodando
ollama list
```

#### 2. Instalar dependÃªncias de experimentaÃ§Ã£o

```bash
pip install -r requirements.txt
```

#### 3. Executar teste rÃ¡pido

```bash
# Teste rÃ¡pido (OpenAI + Ollama se disponÃ­vel)
python src/experiments/run_experiments.py --mode quick

# Benchmark completo (todos os modelos)
python src/experiments/run_experiments.py --mode full

# Abrir MLflow UI
python src/experiments/run_experiments.py --mode ui
```

#### 4. ExperimentaÃ§Ã£o no Jupyter

```bash
# Iniciar Jupyter
jupyter lab

# Abrir: notebooks/experimentacao_llms.ipynb
```

### ğŸ“Š ComparaÃ§Ã£o de Modelos

O sistema de benchmark avalia automaticamente:
- âš¡ **LatÃªncia** - Tempo de resposta
- â­ **Qualidade** - Score de qualidade da resposta
- ğŸ¯ **RelevÃ¢ncia** - PertinÃªncia ao contexto
- ğŸ’° **Custo** - Custo por pergunta (USD)
- ğŸ“ **Tokens** - Uso de tokens

**Exemplo de uso programÃ¡tico:**

```python
from src.experiments.multi_llm import MultiLLMManager
from src.experiments.benchmark import ModelBenchmark

# Criar LLMs
llm_openai = MultiLLMManager.create_llm("openai", "gpt-4o-mini")
llm_ollama = MultiLLMManager.create_llm("ollama", "llama3.1:8b")

# Comparar modelos
benchmark = ModelBenchmark(retriever, memory)
benchmark.add_model("openai", "gpt-4o-mini", llm_openai)
benchmark.add_model("ollama", "llama3.1:8b", llm_ollama)

results = benchmark.run(test_questions)
benchmark.print_report()
```

### ğŸ”¬ MLflow Tracking

Todos os experimentos sÃ£o rastreados automaticamente com MLflow:

```bash
# Visualizar experimentos
mlflow ui --port 5000

# Abrir navegador em: http://localhost:5000
```

**MÃ©tricas rastreadas:**
- ParÃ¢metros do modelo (temperatura, tokens, etc)
- MÃ©tricas de performance (latÃªncia, qualidade)
- ComparaÃ§Ã£o entre runs
- Versionamento de modelos

## ğŸ“Š Tecnologias

### Core RAG
- **LangChain** - Framework RAG e orquestraÃ§Ã£o
- **OpenAI GPT-4o-mini** - Modelo de linguagem
- **OpenAI Embeddings** - Modelo de embeddings (text-embedding-3-small)
- **ChromaDB** - Base de dados vetorial
- **Gradio** - Interface web interativa
- **BeautifulSoup** - Web scraping
- **Pandas** - AnÃ¡lise de dados
- **Docker** - ContainerizaÃ§Ã£o

### ğŸ†• LLMs Open Source & MLOps
- **Ollama** - ExecuÃ§Ã£o local de LLMs (Llama 3.1, Mistral, Phi-3)
- **HuggingFace** - Acesso a modelos open source
- **PyTorch** - Framework de deep learning
- **Transformers** - Biblioteca de modelos
- **PEFT/LoRA** - Fine-tuning eficiente
- **MLflow** - Tracking de experimentos
- **Weights & Biases** - Monitoramento de treinamento

## ğŸ“„ LicenÃ§a

Apache License 2.0 - veja [LICENSE](LICENSE) para detalhes.

## ğŸ¤ ContribuiÃ§Ã£o

Desenvolvido por [Conecta Ads](https://conectaads.com.br)

---

**ğŸ‰ Transformando perguntas em estratÃ©gias de sucesso no Retail Media!**