# üçâ Mel√¢ncIA - Assistente de Marketplace

**Mel√¢ncIA** √© um agente de IA especializado em Product Ads e E-commerce, desenvolvido pela Conecta Ads.

## üöÄ Funcionalidades

- **RAG (Retrieval-Augmented Generation)** com base de conhecimento sobre Retail Media
- **Scraping autom√°tico** de conte√∫do do blog Conecta Ads
- **Interface web** interativa com Gradio
- **Pipeline ETL** completo para processamento de dados
- **An√°lise de conte√∫do** e gera√ß√£o de relat√≥rios
- **Fine-Tuning de LLMs** com QLoRA (4-bit quantization)
- **Evaluation Loops** automatizados com m√∫ltiplas m√©tricas
- **MLflow** para tracking de experimentos

## üõ†Ô∏è Instala√ß√£o

### Pr√©-requisitos

- Python 3.8+
- OpenAI API Key

### Setup Local

```bash
# Clone o reposit√≥rio
git clone https://github.com/conectaads/melancia-ai-rag.git
cd melancia-ai-rag

# Crie e ative o ambiente virtual
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Instale as depend√™ncias
pip install -r requirements.txt

# Configure a API Key
echo "OPENAI_API_KEY=sua_api_key_aqui" > .env
```

## üéØ Como Usar

### Interface Web

```bash
python src/agent/web_interface.py
```

Acesse: http://localhost:8000

### Terminal

```bash
python src/agent/main.py
```

### Pipeline ETL

```bash
# Scraping + An√°lise
python src/etl/run_etl.py

# Apenas scraping
python src/etl/run_etl.py --no-analyze

# Limitar artigos
python src/etl/run_etl.py --max-articles 10
```

## üê≥ Docker

```bash
docker compose up -d
```

## üìÅ Estrutura do Projeto

```text
melancia-ai-rag/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agent/          # Agente RAG principal
‚îÇ   ‚îú‚îÄ‚îÄ etl/            # Pipeline ETL
‚îÇ   ‚îú‚îÄ‚îÄ experiments/    # üÜï Experimenta√ß√£o com LLMs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_llm.py      # Gerenciador de m√∫ltiplos LLMs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ benchmark.py      # Sistema de benchmark
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_experiments.py # Script principal
‚îÇ   ‚îî‚îÄ‚îÄ mlops/          # üÜï MLOps e tracking
‚îÇ       ‚îú‚îÄ‚îÄ tracking.py       # MLflow tracking
‚îÇ       ‚îî‚îÄ‚îÄ registry.py       # Model registry
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/          # Arquivos markdown
‚îÇ   ‚îú‚îÄ‚îÄ output/         # Relat√≥rios e an√°lises
‚îÇ   ‚îú‚îÄ‚îÄ vector_db/      # Base vetorial (ChromaDB)
‚îÇ   ‚îú‚îÄ‚îÄ experiments/    # üÜï Resultados de benchmarks
‚îÇ   ‚îî‚îÄ‚îÄ models/         # üÜï Modelos treinados
‚îú‚îÄ‚îÄ notebooks/          # üÜï Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ experimentacao_llms.ipynb
‚îú‚îÄ‚îÄ mlruns/             # üÜï MLflow tracking data
‚îú‚îÄ‚îÄ logs/               # Logs do sistema
‚îî‚îÄ‚îÄ requirements.txt    # Depend√™ncias
```

## üìù Sistema de Logs

O MelancIA mant√©m um sistema completo de logs:

- **`logs/chat_history.txt`** - Hist√≥rico completo de conversas
- **`data/output/chat_history.pkl`** - Mem√≥ria da conversa (√∫ltimas 5 intera√ß√µes)
- **`logs/etl_pipeline.log`** - Logs do pipeline ETL
- **`logs/scraper.log`** - Logs do web scraping
- **`logs/vector_db.log`** - Logs da base vetorial

> **Nota**: Todos os arquivos de log s√£o ignorados pelo Git (`.gitignore`)

## üß† Como Funciona o RAG

O MelancIA utiliza uma arquitetura RAG (Retrieval-Augmented Generation) sofisticada:

1. **üìö Base de Conhecimento**: Conte√∫do do blog Conecta Ads em formato Markdown
2. **üîç Embeddings**: Transforma o conte√∫do em vetores usando OpenAI embeddings
3. **üíæ Banco Vetorial**: Armazena os vetores no ChromaDB para busca r√°pida
4. **ü§ñ LLM**: GPT-4o-mini gera respostas baseadas no contexto recuperado
5. **üîÑ Mem√≥ria**: Mant√©m contexto das √∫ltimas 5 conversas
6. **üéØ Filtros**: S√≥ responde perguntas relevantes sobre Retail Media

## ü§ñ Sobre o MelancIA

MelancIA √© especializado em:
- **Retail Media** e estrat√©gias de an√∫ncios
- **E-commerce** e marketplaces (Mercado Livre, Shopee)
- **M√©tricas de performance** (ACOS, ROAS, CTR, CPC)
- **Log√≠stica** e fulfillment
- **An√°lise de concorr√™ncia** e tend√™ncias

## üß™ Experimenta√ß√£o com LLMs Open Source

O Mel√¢ncIA agora suporta m√∫ltiplos provedores de LLM para experimenta√ß√£o e compara√ß√£o!

### üíª Hardware Testado

Este projeto foi otimizado para rodar em:
- **CPU**: AMD Ryzen 5 3600X (6 cores, 12 threads)
- **RAM**: 15 GB
- **Disco**: 439 GB (346 GB dispon√≠veis)
- **GPU**: AMD Radeon RX 570/580 (CPU inference recomendado)

‚úÖ **Vi√°vel**: Modelos 2-7B quantizados (Phi-3, Llama 3.2, Gemma)
‚ö†Ô∏è **N√£o recomendado**: Modelos 13B+, GPU training

üìñ **Ver**: [docs/HARDWARE_SETUP.md](docs/HARDWARE_SETUP.md) para an√°lise completa

### Provedores Suportados

**ü§ñ OpenAI** (Pago - Alta Qualidade)
- gpt-4o-mini
- gpt-4o
- gpt-3.5-turbo

**ü¶ô Ollama** (Gratuito - Local)
- llama3.1:8b / llama3.1:70b
- mistral:7b
- phi3:mini
- gemma2:9b
- qwen2.5:7b

**ü§ó HuggingFace** (Gratuito - API)
- mistralai/Mistral-7B-Instruct-v0.2
- meta-llama/Llama-2-7b-chat-hf
- tiiuae/falcon-7b-instruct

### üöÄ Quick Start - Experimentos

#### 1. Instalar Ollama (Recomendado para testes locais)

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo
ollama pull llama3.1:8b

# Verificar se est√° rodando
ollama list
```

#### 2. Instalar depend√™ncias de experimenta√ß√£o

```bash
pip install -r requirements.txt
```

#### 3. Executar teste r√°pido

```bash
# Teste r√°pido (OpenAI + Ollama se dispon√≠vel)
python src/experiments/run_experiments.py --mode quick

# Benchmark completo (todos os modelos)
python src/experiments/run_experiments.py --mode full

# Abrir MLflow UI
python src/experiments/run_experiments.py --mode ui
```

#### 4. Experimenta√ß√£o no Jupyter

```bash
# Iniciar Jupyter
jupyter lab

# Abrir: notebooks/experimentacao_llms.ipynb
```

### üìä Compara√ß√£o de Modelos

O sistema de benchmark avalia automaticamente:
- ‚ö° **Lat√™ncia** - Tempo de resposta
- ‚≠ê **Qualidade** - Score de qualidade da resposta
- üéØ **Relev√¢ncia** - Pertin√™ncia ao contexto
- üí∞ **Custo** - Custo por pergunta (USD)
- üìù **Tokens** - Uso de tokens

**Exemplo de uso program√°tico:**

```python
from src.experiments.multi_llm import MultiLLMManager
from src.experiments.benchmark import ModelBenchmark

# Criar LLMs
llm_openai = MultiLLMManager.create_llm("openai", "gpt-4o-mini")
llm_ollama = MultiLLMManager.create_llm("ollama", "llama3.1:8b")

# Comparar modelos
benchmark = ModelBenchmark(retriever, memory)
benchmark.add_model("openai", "gpt-4o-mini", llm_openai)
benchmark.add_model("ollama", "llama3.1:8b", llm_ollama)

results = benchmark.run(test_questions)
benchmark.print_report()
```

### üî¨ MLflow Tracking

Todos os experimentos s√£o rastreados automaticamente com MLflow:

```bash
# Visualizar experimentos
mlflow ui --port 5000

# Abrir navegador em: http://localhost:5000
```

**M√©tricas rastreadas:**
- Par√¢metros do modelo (temperatura, tokens, etc)
- M√©tricas de performance (lat√™ncia, qualidade)
- Compara√ß√£o entre runs
- Versionamento de modelos

## üéì Fine-Tuning e Evaluation Loops

### Workflow Completo

```
1. Preparar Dados     ‚Üí notebooks/prepare_finetuning_data.ipynb
2. Fine-Tuning        ‚Üí notebooks/fine_tuning_qlora_colab.ipynb
3. Evaluation         ‚Üí notebooks/evaluate_model.ipynb
```

### 1Ô∏è‚É£ Preparar Dataset

```bash
# Preparar dados no formato correto
jupyter notebook notebooks/prepare_finetuning_data.ipynb

# Output: training_dataset/ com splits train/test
```

### 2Ô∏è‚É£ Fine-Tuning (Google Colab)

```bash
# No Google Colab com GPU T4 (gratuito)
1. Abra: notebooks/fine_tuning_qlora_colab.ipynb
2. Configure GPU: Runtime > Change runtime type > T4 GPU
3. Execute c√©lulas para:
   - Carregar modelo com quantiza√ß√£o 4-bit
   - Aplicar LoRA adapters
   - Treinar com seu dataset
   - Salvar modelo fine-tunado
4. Baixe modelo treinado
```

**Otimizado para T4 (15GB VRAM)**:
- Batch size: 1
- Gradient accumulation: 16
- Max sequence length: 1024
- LoRA rank: 8

### 3Ô∏è‚É£ Evaluation Loops

```bash
# Avaliar modelo fine-tunado vs base
jupyter notebook notebooks/evaluate_model.ipynb

# M√©tricas autom√°ticas:
# - ROUGE (overlap de n-gramas)
# - BLEU (qualidade de gera√ß√£o)
# - BERTScore (similaridade sem√¢ntica)
# - Compara√ß√£o lado a lado
# - Visualiza√ß√µes e relat√≥rios
```

**Classe Reutiliz√°vel**:

```python
from src.evaluation.evaluator import ModelEvaluator

# Criar evaluator
evaluator = ModelEvaluator(model, tokenizer, "meu-modelo")

# Avaliar dataset
results = evaluator.evaluate_dataset(test_data)

# Gerar relat√≥rio
report = evaluator.generate_report(results)
```

### üìö Documenta√ß√£o Detalhada

- [üìñ Guia Completo de Fine-Tuning](docs/FINE_TUNING_GUIDE.md)
- [üìä Guia de Evaluation Loops](docs/EVALUATION_GUIDE.md)
- [üî¨ MLOps Report](docs/MLOPS_REPORT.md)

---

## üìä Tecnologias

### Core RAG
- **LangChain** - Framework RAG e orquestra√ß√£o
- **OpenAI GPT-4o-mini** - Modelo de linguagem
- **OpenAI Embeddings** - Modelo de embeddings (text-embedding-3-small)
- **ChromaDB** - Base de dados vetorial
- **Gradio** - Interface web interativa
- **BeautifulSoup** - Web scraping
- **Pandas** - An√°lise de dados
- **Docker** - Containeriza√ß√£o

### üÜï LLMs Open Source & MLOps
- **Ollama** - Execu√ß√£o local de LLMs (Llama 3.1, Mistral, Phi-3)
- **HuggingFace** - Acesso a modelos open source
- **PyTorch** - Framework de deep learning
- **Transformers** - Biblioteca de modelos
- **PEFT/LoRA** - Fine-tuning eficiente
- **MLflow** - Tracking de experimentos
- **Weights & Biases** - Monitoramento de treinamento

## üìÑ Licen√ßa

Apache License 2.0 - veja [LICENSE](LICENSE) para detalhes.

## ü§ù Contribui√ß√£o

Desenvolvido por [Conecta Ads](https://conectaads.com.br)

---

**üçâ Transformando perguntas em estrat√©gias de sucesso no Retail Media!**