# ğŸ‰ MelancIA - Assistente de Marketplace

**MelancIA** Ã© um agente de IA especializado em Product Ads e E-commerce, desenvolvido pela Conecta Ads.

## ğŸš€ Funcionalidades

- **RAG (Retrieval-Augmented Generation)** com base de conhecimento sobre Retail Media
- **Scraping automÃ¡tico** de conteÃºdo do blog Conecta Ads
- **Interface web** interativa com Gradio
- **Pipeline ETL** completo para processamento de dados
- **AnÃ¡lise de conteÃºdo** e geraÃ§Ã£o de relatÃ³rios
- **Fine-Tuning de LLMs** com QLoRA (4-bit quantization)
- **Evaluation Loops** automatizados com mÃºltiplas mÃ©tricas
- **MLflow** para tracking de experimentos

## ğŸ› ï¸ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8+
- OpenAI API Key

### Setup Local

```bash
# Clone o repositÃ³rio
git clone https://github.com/conectaads/melancia-ai-rag.git
cd melancia-ai-rag

# Crie e ative o ambiente virtual
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Instale as dependÃªncias
pip install -r requirements.txt

# Configure a API Key
echo "OPENAI_API_KEY=sua_api_key_aqui" > .env
```

## ğŸ¯ Como Usar

### Interface Web

```bash
python src/agent/web_interface.py
```

Acesse: http://localhost:8000

### Terminal

```bash
python src/agent/main.py
```

### Pipeline ETL

```bash
# Scraping + AnÃ¡lise
python src/etl/run_etl.py

# Apenas scraping
python src/etl/run_etl.py --no-analyze

# Limitar artigos
python src/etl/run_etl.py --max-articles 10
```

## ğŸ³ Docker

```bash
docker compose up -d
```

## ğŸ“ Estrutura do Projeto

```text
melancia-ai-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          # Agente RAG principal
â”‚   â”‚   â”œâ”€â”€ main.py              # CLI do agente
â”‚   â”‚   â”œâ”€â”€ web_interface_with_eval.py  # Interface web com evaluation
â”‚   â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes gerais
â”‚   â”‚   â”œâ”€â”€ keywords.py          # ğŸ†• 120+ keywords organizadas
â”‚   â”‚   â”œâ”€â”€ retriever.py         # ğŸ†• Retriever com MMR
â”‚   â”‚   â”œâ”€â”€ prompt.py            # Templates de prompts
â”‚   â”‚   â”œâ”€â”€ memory.py            # GestÃ£o de memÃ³ria
â”‚   â”‚   â””â”€â”€ utils.py             # FunÃ§Ãµes auxiliares
â”‚   â”œâ”€â”€ llm/            # ğŸ†• Gerenciamento de LLMs
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ manager.py           # MultiLLMManager (OpenAI, Ollama)
â”‚   â”œâ”€â”€ etl/            # Pipeline ETL
â”‚   â”‚   â”œâ”€â”€ scraper_blog_conecta.py
â”‚   â”‚   â”œâ”€â”€ analyzer.py          # AnÃ¡lise de conteÃºdo
â”‚   â”‚   â””â”€â”€ populate_vector_db.py
â”‚   â”œâ”€â”€ experiments/    # ğŸ”¬ ExperimentaÃ§Ã£o e Benchmarks
â”‚   â”‚   â”œâ”€â”€ benchmark.py         # Sistema de benchmark
â”‚   â”‚   â”œâ”€â”€ benchmark_models_mlflow.py  # ğŸ†• Benchmark com MLflow
â”‚   â”‚   â””â”€â”€ run_experiments.py   # Script principal
â”‚   â”œâ”€â”€ evaluation/     # ğŸ†• Sistema de avaliaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ rag_evaluator.py     # MÃ©tricas de qualidade RAG
â”‚   â”‚   â””â”€â”€ interaction_logger.py # Log de interaÃ§Ãµes
â”‚   â”œâ”€â”€ mlops/          # ğŸ†• MLOps e tracking
â”‚   â”‚   â”œâ”€â”€ tracking.py          # MLflow tracking
â”‚   â”‚   â”œâ”€â”€ registry.py          # Model registry
â”‚   â”‚   â””â”€â”€ model_router.py      # Roteamento Ollama/OpenAI
â”‚   â””â”€â”€ monitoring/     # ğŸ†• Monitoramento de Performance
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ latency_monitor.py   # Monitor de latÃªncia
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/          # Arquivos markdown
â”‚   â”‚   â”œâ”€â”€ blog_conecta/         # Blog da Conecta Ads
â”‚   â”‚   â””â”€â”€ central_vendedores/   # Central do Mercado Livre
â”‚   â”œâ”€â”€ output/         # RelatÃ³rios e anÃ¡lises
â”‚   â”œâ”€â”€ vector_db/      # Base vetorial (ChromaDB)
â”‚   â”œâ”€â”€ evaluation/     # ğŸ†• Logs de interaÃ§Ãµes (SQLite)
â”‚   â”œâ”€â”€ experiments/    # ğŸ†• Resultados de benchmarks
â”‚   â””â”€â”€ models/         # ğŸ†• Modelos treinados
â”œâ”€â”€ docs/               # DocumentaÃ§Ã£o detalhada
â”‚   â”œâ”€â”€ AGENT_ARCHITECTURE.md     # Arquitetura do agente
â”‚   â”œâ”€â”€ FINE_TUNING_GUIDE.md
â”‚   â”œâ”€â”€ EVALUATION_GUIDE.md
â”‚   â”œâ”€â”€ HARDWARE_SETUP.md
â”‚   â”œâ”€â”€ MLOPS_REPORT.md
â”‚   â”œâ”€â”€ MONITORING_BEST_PRACTICES.md  # ğŸ†• Monitoramento
â”‚   â”œâ”€â”€ RAG_MEMORY_VS_CACHE.md        # ğŸ†• Como funciona memÃ³ria
â”‚   â””â”€â”€ MODEL_BENCHMARK_GUIDE.md      # ğŸ†• Guia de benchmark
â”œâ”€â”€ notebooks/          # ğŸ†• Jupyter notebooks
â”‚   â””â”€â”€ experimentacao_llms.ipynb
â”œâ”€â”€ mlruns/             # ğŸ†• MLflow tracking data
â”œâ”€â”€ logs/               # Logs do sistema
â””â”€â”€ requirements.txt    # DependÃªncias
```

## ğŸ“ Sistema de Logs

O MelancIA mantÃ©m um sistema completo de logs:

- **`logs/chat_history.txt`** - HistÃ³rico completo de conversas
- **`data/output/chat_history.pkl`** - MemÃ³ria da conversa (Ãºltimas 5 interaÃ§Ãµes)
- **`logs/etl_pipeline.log`** - Logs do pipeline ETL
- **`logs/scraper.log`** - Logs do web scraping
- **`logs/vector_db.log`** - Logs da base vetorial

> **Nota**: Todos os arquivos de log sÃ£o ignorados pelo Git (`.gitignore`)

## ğŸ§  Como Funciona o RAG

O MelancIA utiliza uma arquitetura RAG (Retrieval-Augmented Generation) sofisticada:

1. **ğŸ“š Base de Conhecimento**: ConteÃºdo do blog Conecta Ads + Central de Vendedores do Mercado Livre
2. **ğŸ” Embeddings**: Transforma o conteÃºdo em vetores usando OpenAI embeddings (text-embedding-3-small)
3. **ğŸ’¾ Banco Vetorial**: Armazena os vetores no ChromaDB para busca rÃ¡pida
4. **ğŸ¯ Busca Inteligente (MMR)**: Maximum Marginal Relevance para retornar documentos relevantes E diversos
5. **ğŸ¤– LLM**: GPT-4o-mini gera respostas baseadas no contexto recuperado
6. **ğŸ”„ MemÃ³ria**: MantÃ©m contexto das Ãºltimas 5 conversas
7. **ğŸ›¡ï¸ Filtros Contextuais**: Sistema de keywords para validar relevÃ¢ncia das perguntas

### ğŸ”‘ Sistema de Keywords Expandido

O MelancIA agora possui mais de **120+ keywords** organizadas em categorias:

- **Retail Media & Publicidade**: product ads, anÃºncios patrocinados, display ads
- **MÃ©tricas**: ACOS, TACOS, ROAS, CTR, CPC, visibilidade, ranking
- **CatÃ¡logo**: ficha tÃ©cnica, caracterÃ­sticas, variaÃ§Ãµes, compatibilidade
- **Categorias EspecÃ­ficas**: autopeÃ§as, veÃ­culos, compatibilidade
- **LogÃ­stica**: envio flex, fulfillment, ME1, mesmo dia
- **ReputaÃ§Ã£o**: avaliaÃ§Ãµes, reclamaÃ§Ãµes, tempo de resposta
- **Ferramentas**: excel, editor, planilha, anunciador em massa
- **Branding**: loja oficial, marca, INPI
- **Financeiro**: mercado pago, crÃ©dito, taxas, custos
- **Eventos**: black friday, sazonalidade

Ver lista completa em: `src/agent/keywords.py`

### âš™ï¸ ConfiguraÃ§Ãµes do RAG

O sistema permite ajustes finos de performance:

```python
# src/agent/config.py

RETRIEVER_K = 15  # NÃºmero de documentos recuperados
RETRIEVER_SEARCH_TYPE = "mmr"  # Tipo de busca
# OpÃ§Ãµes: "similarity", "mmr", "similarity_score_threshold"
```

**MMR (Maximum Marginal Relevance)**: Balanceia relevÃ¢ncia e diversidade para evitar documentos redundantes.

## ğŸ¤– Sobre o MelancIA

MelancIA Ã© um assistente especializado em:

- **Retail Media** e estratÃ©gias de anÃºncios
- **E-commerce** e marketplaces (Mercado Livre, Shopee, Amazon)
- **Product Ads** e anÃºncios patrocinados
- **MÃ©tricas de performance** (ACOS, ROAS, CTR, CPC, ROI)
- **CatÃ¡logo de produtos**: ficha tÃ©cnica, caracterÃ­sticas, variaÃ§Ãµes
- **Categorias especÃ­ficas**: autopeÃ§as, compatibilidade de veÃ­culos
- **LogÃ­stica** e fulfillment (Envio Flex, Full, ME1)
- **ReputaÃ§Ã£o e atendimento**: mensagens, avaliaÃ§Ãµes, tempo de resposta
- **Ferramentas de gestÃ£o**: Excel, Editor, Planilha, Anunciador em Massa
- **AnÃ¡lise de concorrÃªncia** e tendÃªncias
- **Black Friday** e sazonalidade

### ğŸ“Š Base de Conhecimento

O assistente possui conhecimento sobre:
- **177+ documentos** do blog Conecta Ads e Central de Vendedores
- ConteÃºdo categorizado por temas (Retail Media, LogÃ­stica, CatÃ¡logo, etc.)
- AtualizaÃ§Ãµes automÃ¡ticas via scraping

## ğŸ§ª ExperimentaÃ§Ã£o com LLMs Open Source

O MelancIA agora suporta mÃºltiplos provedores de LLM para experimentaÃ§Ã£o e comparaÃ§Ã£o!

### ğŸ’» Hardware Testado

Este projeto foi otimizado para rodar em:
- **CPU**: AMD Ryzen 5 3600X (6 cores, 12 threads)
- **RAM**: 15 GB
- **Disco**: 439 GB (346 GB disponÃ­veis)
- **GPU**: AMD Radeon RX 570/580 (CPU inference recomendado)

âœ… **ViÃ¡vel**: Modelos 2-7B quantizados (Phi-3, Llama 3.2, Gemma)
âš ï¸ **NÃ£o recomendado**: Modelos 13B+, GPU training

ğŸ“– **Ver**: [docs/HARDWARE_SETUP.md](docs/HARDWARE_SETUP.md) para anÃ¡lise completa

### Provedores Suportados

**ğŸ¤– OpenAI** (Pago - Alta Qualidade)
- gpt-4o-mini
- gpt-4o
- gpt-3.5-turbo

**ğŸ¦™ Ollama** (Gratuito - Local)
- llama3.1:8b / llama3.1:70b
- mistral:7b
- phi3:mini
- gemma2:9b
- qwen2.5:7b

**ğŸ¤— HuggingFace** (Gratuito - API)
- mistralai/Mistral-7B-Instruct-v0.2
- meta-llama/Llama-2-7b-chat-hf
- tiiuae/falcon-7b-instruct

### ğŸš€ Quick Start - Experimentos

#### 1. Instalar Ollama (Recomendado para testes locais)

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo
ollama pull llama3.1:8b

# Verificar se estÃ¡ rodando
ollama list
```

#### 2. Instalar dependÃªncias de experimentaÃ§Ã£o

```bash
pip install -r requirements.txt
```

#### 3. Executar teste rÃ¡pido

```bash
# Teste rÃ¡pido (OpenAI + Ollama se disponÃ­vel)
python src/experiments/run_experiments.py --mode quick

# Benchmark completo (todos os modelos)
python src/experiments/run_experiments.py --mode full

# Abrir MLflow UI
python src/experiments/run_experiments.py --mode ui
```

#### 4. ExperimentaÃ§Ã£o no Jupyter

```bash
# Iniciar Jupyter
jupyter lab

# Abrir: notebooks/experimentacao_llms.ipynb
```

### ğŸ“Š ComparaÃ§Ã£o de Modelos

O sistema de benchmark avalia automaticamente:
- âš¡ **LatÃªncia** - Tempo de resposta
- â­ **Qualidade** - Score de qualidade da resposta
- ğŸ¯ **RelevÃ¢ncia** - PertinÃªncia ao contexto
- ğŸ’° **Custo** - Custo por pergunta (USD)
- ğŸ“ **Tokens** - Uso de tokens

**Exemplo de uso programÃ¡tico:**

```python
from src.experiments.multi_llm import MultiLLMManager
from src.experiments.benchmark import ModelBenchmark

# Criar LLMs
llm_openai = MultiLLMManager.create_llm("openai", "gpt-4o-mini")
llm_ollama = MultiLLMManager.create_llm("ollama", "llama3.1:8b")

# Comparar modelos
benchmark = ModelBenchmark(retriever, memory)
benchmark.add_model("openai", "gpt-4o-mini", llm_openai)
benchmark.add_model("ollama", "llama3.1:8b", llm_ollama)

results = benchmark.run(test_questions)
benchmark.print_report()
```

### ğŸ”¬ MLflow Tracking

Todos os experimentos sÃ£o rastreados automaticamente com MLflow:

```bash
# Visualizar experimentos
mlflow ui --port 5000

# Abrir navegador em: http://localhost:5000
```

**MÃ©tricas rastreadas:**
- ParÃ¢metros do modelo (temperatura, tokens, etc)
- MÃ©tricas de performance (latÃªncia, qualidade)
- ComparaÃ§Ã£o entre runs
- Versionamento de modelos

## ğŸ“ Fine-Tuning e Evaluation Loops

### Workflow Completo

```
1. Preparar Dados     â†’ notebooks/prepare_finetuning_data.ipynb
2. Fine-Tuning        â†’ notebooks/fine_tuning_qlora_colab.ipynb
3. Evaluation         â†’ notebooks/evaluate_model.ipynb
```

### 1ï¸âƒ£ Preparar Dataset

```bash
# Preparar dados no formato correto
jupyter notebook notebooks/prepare_finetuning_data.ipynb

# Output: training_dataset/ com splits train/test
```

### 2ï¸âƒ£ Fine-Tuning (Google Colab)

```bash
# No Google Colab com GPU T4 (gratuito)
1. Abra: notebooks/fine_tuning_qlora_colab.ipynb
2. Configure GPU: Runtime > Change runtime type > T4 GPU
3. Execute cÃ©lulas para:
   - Carregar modelo com quantizaÃ§Ã£o 4-bit
   - Aplicar LoRA adapters
   - Treinar com seu dataset
   - Salvar modelo fine-tunado
4. Baixe modelo treinado
```

**Otimizado para T4 (15GB VRAM)**:
- Batch size: 1
- Gradient accumulation: 16
- Max sequence length: 1024
- LoRA rank: 8

### 3ï¸âƒ£ Evaluation Loops

```bash
# Avaliar modelo fine-tunado vs base
jupyter notebook notebooks/evaluate_model.ipynb

# MÃ©tricas automÃ¡ticas:
# - ROUGE (overlap de n-gramas)
# - BLEU (qualidade de geraÃ§Ã£o)
# - BERTScore (similaridade semÃ¢ntica)
# - ComparaÃ§Ã£o lado a lado
# - VisualizaÃ§Ãµes e relatÃ³rios
```

**Classe ReutilizÃ¡vel**:

```python
from src.evaluation.evaluator import ModelEvaluator

# Criar evaluator
evaluator = ModelEvaluator(model, tokenizer, "meu-modelo")

# Avaliar dataset
results = evaluator.evaluate_dataset(test_data)

# Gerar relatÃ³rio
report = evaluator.generate_report(results)
```

### ğŸ“š DocumentaÃ§Ã£o Detalhada

- [ğŸ“– Guia Completo de Fine-Tuning](docs/FINE_TUNING_GUIDE.md)
- [ğŸ“Š Guia de Evaluation Loops](docs/EVALUATION_GUIDE.md)
- [ğŸ”¬ MLOps Report](docs/MLOPS_REPORT.md)
- [ğŸ“ Arquitetura do Agente](docs/AGENT_ARCHITECTURE.md)
- [ğŸ“Š Monitoramento e LatÃªncia - Boas PrÃ¡ticas](docs/MONITORING_BEST_PRACTICES.md)
- [ğŸ§  MemÃ³ria RAG vs Cache - Como Funciona](docs/RAG_MEMORY_VS_CACHE.md)
- [ğŸ–¥ï¸ Hardware Setup e LimitaÃ§Ãµes](docs/HARDWARE_SETUP.md)
- [ğŸš€ Guia de Benchmark de Modelos com MLflow](docs/MODEL_BENCHMARK_GUIDE.md)

---

## ğŸ“Š Tecnologias

### Core RAG
- **LangChain** - Framework RAG e orquestraÃ§Ã£o
- **OpenAI GPT-4o-mini** - Modelo de linguagem
- **OpenAI Embeddings** - Modelo de embeddings (text-embedding-3-small)
- **ChromaDB** - Base de dados vetorial
- **Gradio** - Interface web interativa
- **BeautifulSoup** - Web scraping
- **Pandas** - AnÃ¡lise de dados
- **Docker** - ContainerizaÃ§Ã£o

### ğŸ†• LLMs Open Source & MLOps
- **Ollama** - ExecuÃ§Ã£o local de LLMs (Llama 3.1, Mistral, Phi-3)
- **HuggingFace** - Acesso a modelos open source
- **PyTorch** - Framework de deep learning
- **Transformers** - Biblioteca de modelos
- **PEFT/LoRA** - Fine-tuning eficiente
- **MLflow** - Tracking de experimentos
- **Weights & Biases** - Monitoramento de treinamento

## ğŸ“„ LicenÃ§a

Apache License 2.0 - veja [LICENSE](LICENSE) para detalhes.

## ğŸ¤ ContribuiÃ§Ã£o

Desenvolvido por [Conecta Ads](https://conectaads.com.br)

---

**ğŸ‰ Transformando perguntas em estratÃ©gias de sucesso no Retail Media!**