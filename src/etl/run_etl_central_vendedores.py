#!/usr/bin/env python3
"""
Script de execu√ß√£o para o ETL do Central de Vendedores
"""

import sys
from pathlib import Path

# Adicionar o diret√≥rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.etl.scraper_central_vendedores import CentralVendedoresScraper
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def main():
    """Executa o scraping do Central de Vendedores"""
    
    print("\n" + "="*80)
    print("ETL - CENTRAL DE VENDEDORES DO MERCADO LIVRE")
    print("="*80 + "\n")
    
    # Configura√ß√µes
    config = {
        'cursos_url': 'https://central.vendedores.mercadolivre.com.br/cursos',
        'output_dir': 'data/input/central_vendedores_new',
        'use_selenium': True,
        'max_courses': 2,  # Apenas 2 cursos para teste inicial
        'extract_details': False  # N√£o extrair detalhes por enquanto (mais r√°pido)
    }
    
    print("Configura√ß√µes:")
    print(f"  - URL base: {config['cursos_url']}")
    print(f"  - Diret√≥rio de sa√≠da: {config['output_dir']}")
    print(f"  - Usar Selenium: {config['use_selenium']}")
    print(f"  - Max cursos (teste): {config['max_courses']}")
    print(f"  - Extrair detalhes: {config['extract_details']}")
    print()
    
    resposta = input("Deseja continuar? (s/n): ").strip().lower()
    if resposta != 's':
        print("Cancelado pelo usu√°rio.")
        return
    
    # Criar scraper
    scraper = CentralVendedoresScraper(
        cursos_url=config['cursos_url'],
        output_dir=config['output_dir'],
        use_selenium=config['use_selenium']
    )
    
    try:
        # Executar scraping
        stats = scraper.scrape_all(
            max_courses=config['max_courses'],
            extract_details=config['extract_details']
        )
        
        # Mostrar resultados
        print("\n" + "="*80)
        print("RESULTADOS")
        print("="*80)
        print(f"‚úì Cursos processados: {stats.get('processed_courses', 0)}")
        print(f"‚úì Cursos com sucesso: {stats.get('successful_courses', 0)}")
        print(f"‚úó Cursos com falha: {stats.get('failed_courses', 0)}")
        print(f"üìö Total de m√≥dulos: {stats.get('total_modules', 0)}")
        print(f"üìÑ Total de conte√∫dos: {stats.get('total_contents', 0)}")
        
        # Salvar relat√≥rio
        import json
        report_file = Path(config['output_dir']) / 'scraping_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìä Relat√≥rio salvo em: {report_file}")
        print(f"üìÅ Arquivos salvos em: {config['output_dir']}")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Processo interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n\n‚ùå Erro fatal: {e}")
        import traceback
        traceback.print_exc()
    finally:
        scraper.close()
        print("\n‚úì Recursos liberados")


if __name__ == "__main__":
    main()

